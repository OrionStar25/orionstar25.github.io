<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Functions, Tools, and Agents | Niharika Shrivastava</title> <meta name="author" content="Niharika Shrivastava"> <meta name="description" content="Understand the capabilities of LLMs to the fullest."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, niharika-shrivastava"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%AF%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://orionstar25.github.io/blog/2024/function-calling/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Niharika </span>Shrivastava</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">opensource</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/presentations/">presentations</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/contributions/">contributions</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/repositories/">github</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/checklist/">my dynamic checklist</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Functions, Tools, and Agents</h1> <p class="post-meta">November 1, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/opensource"> <i class="fa-solid fa-hashtag fa-sm"></i> opensource</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/github"> <i class="fa-solid fa-hashtag fa-sm"></i> github</a>     ·   <a href="/blog/category/open-source"> <i class="fa-solid fa-tag fa-sm"></i> open-source</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="function-calling">Function Calling</h1> <p>Function calling in large language models (LLMs) is a technique that allows LLMs to interact with external tools and APIs by:</p> <ol> <li> <p><strong>Detecting when a function is needed:</strong> LLMs are fine-tuned to identify when a function needs to be called based on a user’s prompt.</p> </li> <li> <p><strong>Generating a structured response:</strong> If applicable, LLMs generate a JSON object that contains the function name and arguments.</p> </li> <li> <p><strong>Executing the function:</strong> The developer’s code executes the function using the extracted arguments and to get an output.</p> </li> <li> <p><strong>Using the function output:</strong> The LLM uses the function output to generate a final response in natural language to the user.</p> </li> </ol> <p>In this post, I will explain the common structure of function-calling, tools, and agents using LangChain and OpenAI models.</p> <h2 id="langchain-expression-language-lcel">LangChain Expression Language (LCEL)</h2> <p>LangChain is an open-source framework that allows developers to build applications using large language models (LLMs). The LangChain Expression Language (LCEL) helps to compose a chain of LangChain components with minimal code.</p> <p>Typically, a chain contains the following components seperated by the <code class="language-plaintext highlighter-rouge">|</code> operator:</p> <blockquote> <table> <tbody> <tr> <td>Prompt</td> <td>LLM</td> <td>OutputParser</td> </tr> </tbody> </table> </blockquote> <p>Below is a code example to use LCEL and an LLM to generate a chain.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser

# Create a prompt
prompt = ChatPromptTemplate.from_template(
    "tell me a short joke about {topic}"
)

# Instantiate an LLM
model = ChatOpenAI()

# Instantiate a parser that converts model output to string
output_parser = StrOutputParser()

# Create a langChain "chain" of components using LCEL
chain = prompt | model | output_parser

# Invoke the chain to generate a response from the LLM
chain.invoke({"topic": "bears"})
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Why do bears have hairy coats?\n\nBecause they don't like to shave!"
</code></pre></div></div> <ol> <li>The user creates a string prompt.</li> <li>This prompt is sent to the LLM to generate an output.</li> <li>The LLM output object is properly parsed to generate a string response for the user.</li> </ol> <h2 id="bind-custom-functions-to-llms">Bind custom functions to LLMs</h2> <p>The purpose of function-calling is:</p> <ol> <li>To leverage an LLM to analyse a a user query in order to determine whether a function-call is required to generate the desired response.</li> <li>If a function call is required, the LLM needs to figure out the correct function to call along with the required function argument values.</li> </ol> <p>This process helps the developer to then execute the relevant function with the arguments to generate the desired response. A function could be a custom function written by the developer or an API call that generates an output based on given arguments.</p> <p>OpenAI models expect the functions to be defined in the following format:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>functions = [
   {
      "name": &lt;function_name&gt;,
      "description": &lt;function description&gt;,
      "parameters": {
        "type": "object",
        "properties": {
          &lt;argument_name&gt;: {
            "type": &lt;argument_type&gt;,
            "description": &lt;argument_description&gt;
          },
        },
        "required": [&lt;list of required arguments&gt;]
      }
   },
...
]
</code></pre></div></div> <p>Let’s take an example with 2 custom defined functions <code class="language-plaintext highlighter-rouge">weather_search</code> and <code class="language-plaintext highlighter-rouge">sports_search</code>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Define the functions
functions = [
    {
      "name": "weather_search",
      "description": "Search for weather given an airport code",
      "parameters": {
        "type": "object",
        "properties": {
          "airport_code": {
            "type": "string",
            "description": "The airport code to get the weather for"
          },
        },
        "required": ["airport_code"]
      }
    },
        {
      "name": "sports_search",
      "description": "Search for news of recent sport events",
      "parameters": {
        "type": "object",
        "properties": {
          "team_name": {
            "type": "string",
            "description": "The sports team to search for"
          },
        },
        "required": ["team_name"]
      }
    }
  ]

# Create a prompt
prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}")
    ]
)

# Instantiate an LLM and "bind" the defined functions with the LLM.
# This tells the LLM the list of available functions to choose from in case function-calling is required.
model = ChatOpenAI(temperature=0).bind(functions=functions)

# Create a chain
runnable = prompt | model
</code></pre></div></div> <p>Invoke the chain with a sports-related question:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>runnable.invoke({"input": "how did the patriots do yesterday?"})
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'sports_search', 'arguments': '{"team_name":"New England Patriots"}'}})
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">content</code> is empty because the LLM determined that a function needs to be called in order to generate a response.</li> <li> <code class="language-plaintext highlighter-rouge">function_call</code> containts the following information: <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>'function_call': {
 'name': 'sports_search', # the custom function that needs to be called 
 'arguments': '{"team_name":"New England Patriots"}' # the arguments to the function.
}
</code></pre></div> </div> <p>The LLM automatically determined that <code class="language-plaintext highlighter-rouge">patriots</code> in the user query referred to the <code class="language-plaintext highlighter-rouge">New England Patriots</code> which is a sports team. Interesting!</p> </li> </ul> <p>Now, let’s invoke the chain with a weather related question:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>runnable.invoke({"input": "what is the weather in sf"})
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'weather_search', 'arguments': '{"airport_code":"SFO"}'}})
</code></pre></div></div> <p>This time, the LLM determined that the function <code class="language-plaintext highlighter-rouge">weather_search</code> needs to be called with the argument <code class="language-plaintext highlighter-rouge">airport_code=SFO</code> where <code class="language-plaintext highlighter-rouge">SFO</code> stands for San Francisco!</p> <p>By default, if none of the defined functions are relevant to the user query, the LLM will not invoke function-calling and generate a normal user (natural language) response. It doesn’t forcibly use any function or hallucinate its arguments - until explicitely made to do so.</p> <h2 id="use-pydantic-to-create-functions-with-ease">Use pydantic to create functions with ease</h2> <p>Pydantic is a Python library for data validation using Python-type annotations. It ensures that the data you work with matches your specified data types, simplifying error handling and data parsing in Python applications.</p> <p>Thus, Pydantic will help us define LLM functions with ease. Let’s look at an example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Define a Pydantic class which refers to a function
# The fields of the class refer to function arguments
# The docstring used to describe the class is mandatory as it helps the LLM understand what the purpose of the function is. 

class WeatherSearch(BaseModel):
    """Call this with an airport code to get the weather at that airport"""
    airport_code: str = Field(description="airport code to get weather for")

from langchain.utils.openai_functions import convert_pydantic_to_openai_function
convert_pydantic_to_openai_function(WeatherSearch)
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'name': 'WeatherSearch', # name of the class
 'description': 'Call this with an airport code to get the weather at that airport', # Pulled from the docstring used to describe the class.
 'parameters': {'title': 'WeatherSearch',
  'description': 'Call this with an airport code to get the weather at that airport',
  'type': 'object',
  'properties': {'airport_code': {'title': 'Airport Code',
    'description': 'airport code to get weather for',
    'type': 'string'}},
  'required': ['airport_code']}}
</code></pre></div></div> <p>Let’s create multiple functions using Pydantic!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Custom function with 2 args
class ArtistSearch(BaseModel):
    """Call this to get the names of songs by a particular artist"""
    artist_name: str = Field(description="name of artist to look up")
    n: int = Field(description="number of results")

functions = [
    convert_pydantic_to_openai_function(WeatherSearch),
    convert_pydantic_to_openai_function(ArtistSearch),
]

# Bind the functions to the LLM
model_with_functions = model.bind(functions=functions)
</code></pre></div></div> <p>Invoking with weather related question:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_with_functions.invoke("what is the weather in sf?")
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'WeatherSearch', 'arguments': '{"airport_code":"SFO"}'}})
</code></pre></div></div> <p>Invoking with artist related question:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_with_functions.invoke("what are three songs by taylor swift?")
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AIMessage(content='', additional_kwargs={'function_call': {'name': 'ArtistSearch', 'arguments': '{"artist_name":"Taylor Swift","n":3}'}})
</code></pre></div></div> <p>Default behaviour:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model_with_functions.invoke("hi!")
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AIMessage(content='Hello! How can I assist you today?')
</code></pre></div></div> <h2 id="extraction">Extraction</h2> <p>Concept extraction is an NLP task that automatically identifies and extracts specific concepts or entities from unstructured text.</p> <p>Let’s see how function-calling can help us solve this task of extracting all the papers and their respective authors mentioned in an unstructured article with ease.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Load unstructured text from the internet
from langchain.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
documents = loader.load()
page_content = documents[0].page_content[:10000]

# Create a prompt
template = """A article will be passed to you. Extract from it all papers that are mentioned by this article followed by its author. 

Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.

Do not make up or guess ANY extra information. Only extract what exactly is in the text."""

prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", "{input}")
])

# Create custom function definitions
class Paper(BaseModel):
    """Information about papers mentioned."""
    title: str
    author: Optional[str]

class Info(BaseModel):
    """Information to extract"""
    papers: List[Paper]

paper_extraction_function = [
    convert_pydantic_to_openai_function(Info)
]

# Bind the function to the LLM
extraction_model = model.bind(
    functions=paper_extraction_function, # functions present for binding
    function_call={"name":"Info"} # Force the LLM to use the function "Info" everytime
)

# Create a chain and invoke it with the article
extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name="papers")
extraction_chain.invoke({"input": page_content})
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': 'Wei et al. 2022'},
 {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': 'Yao et al. 2023'},
 {'title': 'LLM+P (Liu et al. 2023)', 'author': 'Liu et al. 2023'},
 {'title': 'ReAct (Yao et al. 2023)', 'author': 'Yao et al. 2023'},
 {'title': 'Reflexion (Shinn &amp; Labash 2023)', 'author': 'Shinn &amp; Labash 2023'},
 {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)', 'author': 'Liu et al. 2023'},
 {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)', 'author': 'Laskin et al. 2023'}]
</code></pre></div></div> <p>The chain has successfully extracted all the papers that were mentioned in the article along with its authors in a structured JSON format.</p> <p>That’s all for function-calling (for now)!</p> <h1 id="tools">Tools</h1> <h1 id="agents">Agents</h1> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/outreachy-week-3/">Let's hunt those Memory Leaks!</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/outreachy-week-7/">Season @ Fedora Modularity</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/my-milky-way/">My Milky Way</a> </li> <div id="giscus_thread" style="max-width: 1200px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"OrionStar25/orionstar25.github.io","data-repo-id":"R_kgDOK7Nn4Q","data-category":"Announcements","data-category-id":"DIC_kwDOK7Nn4c4Ce3KD","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Niharika Shrivastava. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>