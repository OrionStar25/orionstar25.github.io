<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://orionstar25.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://orionstar25.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-23T08:10:43+00:00</updated><id>https://orionstar25.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Privacy Preserving AI</title><link href="https://orionstar25.github.io/blog/2024/fl-challenge-day-2/" rel="alternate" type="text/html" title="Privacy Preserving AI"/><published>2024-11-22T00:00:00+00:00</published><updated>2024-11-22T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/fl-challenge-day-2</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/fl-challenge-day-2/"><![CDATA[<p>It‚Äôs Day 2 of the <a href="https://info.openmined.org/30daysofflcode">#30DaysOfFLCode</a> Challenge by <a href="https://openmined.org/">OpenMined</a> ü•≥!</p> <p>Today I went through the video: <a href="https://www.youtube.com/watch?v=4zrU54VIK6k">Privacy Preserving AI by Andrew Trask: MIT Deep Learning Series</a>.</p> <p>Below are my notes from the talk ‚¨áÔ∏è.</p> <h1 id="tools-for-remote-data-science-">Tools for Remote Data Science üìâ</h1> <blockquote> <p>Can we answer questions using data we cannot see?</p> </blockquote> <p>There are multiple tools for being able to use the data we donot have to answer important questions.</p> <h2 id="tool-1-remote-execution">Tool 1: Remote Execution</h2> <p><a href="https://github.com/OpenMined/PySyft">PySyft</a> is a library that helps to coordinate with data and computations remotely. It is a wrapper over PyTorch and has the ability to instantiate a Virtual Worker where all the computation will happen.</p> <h2 id="tool-2-search-and-example-data">Tool 2: Search and Example Data</h2> <p><a href="https://blog.openmined.org/what-is-pygrid-demo/">PyGrid</a> is a platform that:</p> <ul> <li>helps to search and access snippets of actual data,</li> <li>answer relevant statistical questions regarding the data distribution,</li> </ul> <p>such that it will help later to perform pre-processing locally. This enables individual parties to do basic feature engineering with sample data.</p> <p>However, there is a possibility of stealing/reconstructing sensitive data using the <code class="language-plaintext highlighter-rouge">.get()</code> method (sample data).</p> <h2 id="tool-3-differential-privacy">Tool 3: Differential Privacy</h2> <p>Differential Privacy is a framework that ensures an algorithm‚Äôs output does not reveal whether any specific individual‚Äôs data was included in the input dataset. Formally, a mechanism is $(\epsilon, \delta)$-differentially private if, for any two datasets $D_1$ and $D_2$ differing in at most one record, and for all possible outputs $S$:</p> \[[ P[\mathcal{M}(D_1) \in S] \leq e^\epsilon \cdot P[\mathcal{M}(D_2) \in S] + \delta ]\] <p>This guarantees that the output remains statistically similar regardless of an individual‚Äôs participation, ensuring privacy.</p> <h3 id="sensitivity">Sensitivity:</h3> <p>Sensitivity quantifies the maximum change in an algorithm‚Äôs output due to the inclusion or exclusion of a single data point in the dataset. It is defined as:</p> \[[ \Delta f = \max_{D_1, D_2} ||f(D_1) - f(D_2)|| ]\] <p>where $f$ is the query or function, and $D_1, D_2$ are datasets differing in one record. Lower sensitivity functions make it easier to achieve differential privacy.</p> <h3 id="epsilon-epsilon-privacy-budget">Epsilon $\epsilon$ (Privacy Budget):</h3> <p>Epsilon is a parameter that measures the privacy loss in a differentially private mechanism. Smaller values of $\epsilon$ provide stronger privacy guarantees but reduce the accuracy of the results. Conversely, larger $\epsilon$ allows better accuracy but weaker privacy.</p> <p>The ‚Äúprivacy budget‚Äù refers to the cumulative $\epsilon$ used across multiple queries; once it‚Äôs exhausted, the mechanism risks breaching privacy guarantees.</p> <h3 id="types-of-differential-privacy">Types of Differential Privacy:</h3> <h4 id="local-differential-privacy-ldp">Local Differential Privacy (LDP):</h4> <p>LDP ensures privacy at the individual data level, meaning each data contributor perturbs their model weights before sharing it with the server. The server never sees raw data. This approach requires:</p> <ul> <li>Trust in the data contributors.</li> <li>Strong privacy guarantees since no raw data is exposed.</li> </ul> <p><strong>Best use case:</strong> Scenarios where users want to ensure absolute privacy without trusting the server, such as in surveys or telemetry data collection.</p> <p><strong>Limitation:</strong> Results may be noisier compared to global DP due to individual perturbation.</p> <h4 id="global-differential-privacy-centralized-dp">Global Differential Privacy (Centralized DP):</h4> <p>In global DP, raw data is collected centrally, and noise is added at the aggregate level (e.g., during analysis). The server is assumed to be trusted not to misuse raw data.</p> <p><strong>Advantages:</strong></p> <ul> <li>Produces more accurate results since perturbation happens after aggregation.</li> <li>Better for large-scale analysis, like training machine learning models.</li> </ul> <p><strong>Risk:</strong> Requires trusting the server, which could potentially compromise the raw data.</p> <p><strong>Best Way to Use Local DP vs Global DP:</strong></p> <ul> <li><strong>Local DP:</strong> Use when trust in the server is low, or contributors demand high privacy.</li> <li><strong>Global DP:</strong> Use when high accuracy is critical, and there is sufficient trust in the central server‚Äôs security.</li> </ul> <blockquote> <p>‚ÄúMultiple companies‚Äô entire business model is buy anonymized datasets, merge them together to de-anonymize them, and sell market intelligence to insurance companies‚Äù. - Andrew Trask</p> </blockquote> <p>The idea of the tools is to allow a set no. of queries that do not exceed the epsilon (privacy budget). This will give privacy guarantees for the sensitive dataset.</p> <h3 id="who-sets-the-privacy-budget">Who sets the privacy budget?</h3> <ul> <li> <p>Not the data scientist because thats a conflict of interest.</p> </li> <li>Potentially data owners (e.g. hospitals), but this still leads back to the same issue; <ul> <li>1 hospital releases 0.1 eps of data,</li> <li>another releases 0.1 eps of separate data,</li> <li>total publicly available data of a common user = 0.2.</li> <li>imagine if eps budget is 0.15, this is already exceeded ‚Äì&gt; privacy is lost!</li> </ul> </li> <li>Ideally individuals should be allowed to set the budget. However, it‚Äôs too far a goal for now.</li> </ul> <p>However, a few issues with DP are:</p> <ul> <li>we cannot do join computations anymore inherently due to the statistically added noise.</li> <li>the data is safe but the model can be corrupted by individual parties.</li> <li>need to trust the remote server for not corrupting the model by doing something malicious.</li> </ul> <h2 id="tool-4-secure-multi-party-computation">Tool 4: Secure Multi-Party Computation</h2> <p>Its a cryptographic protocol that allows multiple parties to jointly compute a function over their inputs while keeping those inputs private. None of the parties learns anything about the other participants‚Äô inputs except what can be inferred from the output of the computation.</p> <p>In SMPC, each participant‚Äôs data is split into encrypted or obfuscated shares and distributed among the parties. The computation is then carried out on these shares without revealing the underlying data. After computation, the final result is reconstructed, ensuring:</p> <ul> <li><strong>Privacy</strong>: No party learns anything about the private inputs except the output.</li> <li><strong>Correctness</strong>: The result of the computation is accurate and consistent with the function.</li> <li><strong>Security</strong>: Colluding subsets of participants (below a certain threshold) cannot learn private information.</li> </ul> <blockquote> <p>The implication is that multiple people can share ownership of a number. And models and datasets are just large collections of numbers which we can encrypt.</p> </blockquote> <p>This provides 2 benefits:</p> <ol> <li><strong>Encryptions:</strong> Neither knows the hidden value.</li> <li><strong>Shared Governance</strong>: The number can only be decrypted till all of the shareholders agree.</li> </ol> <h4 id="advantages">Advantages</h4> <ul> <li>Models can be encrypted during training</li> <li>We can do joins/ functions across data owners.</li> </ul> <h4 id="disadvantages">Disadvantages</h4> <ul> <li>It is computationally expensive.</li> </ul> <h1 id="ai-privacy-society">AI, Privacy, Society</h1> <p>3 broad non-exhaustive categories are important for the future of AI.</p> <h2 id="1-open-data-for-science">1. Open Data for Science</h2> <blockquote> <p>‚ÄúEveryone‚Äôs gonna work on models, but if you look historically, the biggest jumps in progress have happened when we had new big datasets - or the ability to process them.‚Äù - Phil Blunsom</p> </blockquote> <h2 id="2-single-use-accountability">2. Single-Use Accountability</h2> <p>Build systems that are able to answer questions you really need to answer and nothing else. This makes it privacy preserving.</p> <p>E.g., Using raw data to answer what causes cancer</p> <ul> <li>while not knowing who all have it</li> <li>while not being able to answer any other query other than the objective</li> </ul> <p>is privacy-preserving.</p> <h2 id="3-end-to-end-encrypted-services">3. End-to-End Encrypted Services</h2> <p>Provide services (model computations and forecasts) without the need for knowing individual data is privacy-preserving.</p> <ul> <li>Models can be encrypted.</li> <li>Data can be encrypted.</li> </ul> <hr/> <p>That is all for today folks! Blogging + learning is a long process haha, but I‚Äôm glad I learnt a bunch today! See you tomorrow :D</p> <h1 id="references">References</h1> <p>[1] <a href="https://www.youtube.com/watch?v=4zrU54VIK6k">https://www.youtube.com/watch?v=4zrU54VIK6k</a></p>]]></content><author><name></name></author><category term="open-source"/><category term="opensource"/><category term="ai"/><category term="federated-learning"/><category term="openmined"/><category term="challenge"/><category term="data-privacy"/><category term="trustworthy-ai"/><summary type="html"><![CDATA[It‚Äôs Day 2 of the #30DaysOfFLCode Challenge by OpenMined ü•≥!]]></summary></entry><entry><title type="html">Introduction to Federated Learning</title><link href="https://orionstar25.github.io/blog/2024/fl-challenge-day-1/" rel="alternate" type="text/html" title="Introduction to Federated Learning"/><published>2024-11-21T00:00:00+00:00</published><updated>2024-11-21T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/fl-challenge-day-1</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/fl-challenge-day-1/"><![CDATA[<p>It‚Äôs Day 1 of the <a href="https://info.openmined.org/30daysofflcode">#30DaysOfFLCode</a> Challenge by <a href="https://openmined.org/">OpenMined</a> ü•≥!</p> <p>Today I went through an introductory tutorial on Federated Learning by <a href="https://learn.deeplearning.ai/">DeepLearning.AI</a> and <a href="https://flower.ai/">Flower Labs</a>: <a href="https://learn.deeplearning.ai/courses/intro-to-federated-learning/">Intro to Federated Learning</a>.</p> <p>Let‚Äôs quickly learn a few concepts ‚¨áÔ∏è.</p> <h1 id="introduction-">Introduction üìâ</h1> <h2 id="why-is-federated-learning-needed">Why is federated learning needed?</h2> <ol> <li> <p>Llama-3 is trained on 15 trillion tokens which happens to be very close to the amount of publicly available data. According to a study, we will run out of new publicly available data to further train our models by 2026. In contrast, about 150 trillion tokens are expected to be present in all of the private data globally (e.g., in text messages and emails). This data is not freely accessible for model training.</p> </li> <li> <p>All the data is naturally distributed across industries, geographic locations, devices. For e.g.,</p> <ul> <li>tons of medical data is distributed among different hospitals.</li> <li>individual data is distributed amongst mobile, laptop, home assistants, etc.</li> </ul> </li> </ol> <p>Traditional data science methods require all the data to be centrally placed during training. Collecting more data doesn‚Äôt work for multiple reasons:</p> <ul> <li><strong>Sensitive data:</strong> Data can be either sensitve or bound by regulations barring it from being used/moved freely.</li> <li><strong>Data volume:</strong> It can be huge which might cause constraints on storage and computation.</li> <li><strong>Practicality:</strong> It might not be feasible to collect a lot of data often.</li> </ul> <p>To address these issues, AI models started to shift to a decentralized approach, and a new concept called ‚Äúfederated learning‚Äù has emerged.</p> <p>Federated learning (often referred to as collaborative learning) is a decentralized approach to training machine learning models. <strong>It brings the training process to the data than data to the training.</strong> It doesn‚Äôt require an exchange of data from clients to centralised servers. Organizations retain full control over their data.</p> <p>A few examples of where its used:</p> <ol> <li> <p><strong>Global model for anti-money laundering model:</strong> Build a global model to detect money-laundering activity using the customer transcations in USA and EU without moving the data from their respective geographic locations. This makes sure the respective data regulations are followed without compromising model training.</p> </li> <li> <p><strong>Google GBoard on Android:</strong> A popular example wherein it is deployed on millions of edge devices to predict the next word based on each user‚Äôs data pattern. The model uses FL to learn accurate predictions without needing the data of each user.</p> </li> </ol> <h2 id="bad-generalization-capabilities-due-to-lack-of-complete-data-distribution">Bad Generalization Capabilities due to lack of Complete Data Distribution</h2> <p>Say 3 people have 3 different data distributions of the MNIST dataset each with a few digits missing.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fl-mnist-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fl-mnist-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fl-mnist-1400.webp"/> <img src="/assets/img/fl-mnist.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By training 3 different models on the these datasets, we observe the average accuracy of all 3 models to be <code class="language-plaintext highlighter-rouge">~67%</code>. Morevover, the accuracy on specifically the missing data for each model is <code class="language-plaintext highlighter-rouge">0%</code>. This is not a good performing model at all - and in fact a very real challenge.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model 1-&gt; 
  Test Accuracy on all digits: 0.6570, 
  Test Accuracy on [1,3,7]: 0.0000

Model 2-&gt; 
  Test Accuracy on all digits: 0.6876, 
  Test Accuracy on [2,5,8]: 0.0000

Model 3-&gt; 
  Test Accuracy on all digits: 0.6848, 
  Test Accuracy on [4,6,9]: 0.0000
</code></pre></div></div> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/model1_cm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/model1_cm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/model1_cm-1400.webp"/> <img src="/assets/img/model1_cm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Clearly, the models perform very bad on data it has never seen.</p> <p>Thus, we‚Äôll now see a big advantage of FL is that all parties can learn from each others data without compromising any private data leakage between them.</p> <h1 id="federated-learning-process">Federated Learning Process</h1> <p><img src="https://framerusercontent.com/images/pGx10j45WZIitkcA6GjZ2SITMw.webp" alt=""/></p> <h2 id="algorithm">Algorithm</h2> <ol> <li> <p><strong>Initialization:</strong> Server initializes the global model.</p> </li> <li><strong>Communication Round:</strong> For each round: <ul> <li>Server sends the global model to the participating clients.</li> <li>Each client receives the global model.</li> </ul> </li> <li><strong>Client Training and Model Update:</strong> For each participating client: <ul> <li>Client trains the received model on its local dataset <ul> <li>usually for 1-2 epochs.</li> <li>overfitting the model on 1 client is bad for the process.</li> </ul> </li> <li>Client sends its locally updated model to the server.</li> </ul> </li> <li> <p><strong>Model Aggregation:</strong> Server aggregates the updated models received from all the clienrs using an Aggregating strategy (e.g., FedAvg).</p> </li> <li><strong>Convergence Check:</strong> <ul> <li>If convergence critiria is met, end the FL process.</li> <li>If not, procedd to the nect communication round (step 2).</li> </ul> </li> </ol> <h2 id="implement-using-flower">Implement using Flower</h2> <ol> <li>Download relevant data.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flwr.client</span> <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">ClientApp</span><span class="p">,</span> <span class="n">NumPyClient</span>
<span class="kn">from</span> <span class="n">flwr.common</span> <span class="kn">import</span> <span class="n">ndarrays_to_parameters</span><span class="p">,</span> <span class="n">Context</span>
<span class="kn">from</span> <span class="n">flwr.server</span> <span class="kn">import</span> <span class="n">ServerApp</span><span class="p">,</span> <span class="n">ServerConfig</span>
<span class="kn">from</span> <span class="n">flwr.server</span> <span class="kn">import</span> <span class="n">ServerAppComponents</span>
<span class="kn">from</span> <span class="n">flwr.server.strategy</span> <span class="kn">import</span> <span class="n">FedAvg</span>
<span class="kn">from</span> <span class="n">flwr.simulation</span> <span class="kn">import</span> <span class="n">run_simulation</span>


<span class="c1"># download mnist dataset
</span><span class="n">trainset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">./MNIST_data/</span><span class="sh">"</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>

<span class="c1"># split into 3 different parts to simulate different clients
</span><span class="n">total_length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span>
<span class="n">split_size</span> <span class="o">=</span> <span class="n">total_length</span> <span class="o">//</span> <span class="mi">3</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">part1</span><span class="p">,</span> <span class="n">part2</span><span class="p">,</span> <span class="n">part3</span> <span class="o">=</span> <span class="nf">random_split</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="p">[</span><span class="n">split_size</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">part1</span> <span class="o">=</span> <span class="nf">exclude_digits</span><span class="p">(</span><span class="n">part1</span><span class="p">,</span> <span class="n">excluded_digits</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">part2</span> <span class="o">=</span> <span class="nf">exclude_digits</span><span class="p">(</span><span class="n">part2</span><span class="p">,</span> <span class="n">excluded_digits</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">part3</span> <span class="o">=</span> <span class="nf">exclude_digits</span><span class="p">(</span><span class="n">part3</span><span class="p">,</span> <span class="n">excluded_digits</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>

<span class="n">train_sets</span> <span class="o">=</span> <span class="p">[</span><span class="n">part1</span><span class="p">,</span> <span class="n">part2</span><span class="p">,</span> <span class="n">part3</span><span class="p">]</span>

<span class="c1"># Common testset
</span><span class="n">testset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">./MNIST_data/</span><span class="sh">"</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>

<span class="n">testset_137</span> <span class="o">=</span> <span class="nf">include_digits</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">testset_258</span> <span class="o">=</span> <span class="nf">include_digits</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">testset_469</span> <span class="o">=</span> <span class="nf">include_digits</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
</code></pre></div></div> <ol> <li>Define the following functions. <ul> <li><code class="language-plaintext highlighter-rouge">set_weights</code>: each client uses it to set the global params to the individual model.</li> <li><code class="language-plaintext highlighter-rouge">get_weights</code>: each client retrieves the model weight after individual training and returns to server.</li> </ul> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sets the parameters of the model
</span><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="n">params_dict</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">().</span><span class="nf">keys</span><span class="p">(),</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="n">state_dict</span> <span class="o">=</span> <span class="nc">OrderedDict</span><span class="p">(</span>
        <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="n">net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Retrieves the parameters from the model
</span><span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="n">ndarrays</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">val</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">().</span><span class="nf">items</span><span class="p">()</span>
    <span class="p">]</span>
    <span class="k">return</span> <span class="n">ndarrays</span>
</code></pre></div></div> <ol> <li>Create a <code class="language-plaintext highlighter-rouge">FlowerClient</code> to train and evaluate each client model. Flower calls <code class="language-plaintext highlighter-rouge">client_fn</code> whenever it needs an instance of one particular client to call fit or evaluate.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FlowerClient</span><span class="p">(</span><span class="n">NumPyClient</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
        <span class="n">self</span><span class="p">.</span><span class="n">trainset</span> <span class="o">=</span> <span class="n">trainset</span>
        <span class="n">self</span><span class="p">.</span><span class="n">testset</span> <span class="o">=</span> <span class="n">testset</span>

    <span class="c1"># Train the model
</span>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">set_weights</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">net</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="nf">train_model</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">net</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">trainset</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">get_weights</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">net</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">trainset</span><span class="p">),</span> <span class="p">{}</span>

    <span class="c1"># Test the model
</span>    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">NDArrays</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Scalar</span><span class="p">]):</span>
        <span class="nf">set_weights</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">net</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">net</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">testset</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">testset</span><span class="p">),</span> <span class="p">{</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">}</span>


<span class="c1"># Client function
</span><span class="k">def</span> <span class="nf">client_fn</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="n">Context</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Client</span><span class="p">:</span>
    <span class="n">net</span> <span class="o">=</span> <span class="nc">SimpleModel</span><span class="p">()</span> <span class="c1"># initialise a model for each client
</span>    <span class="n">partition_id</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="n">node_config</span><span class="p">[</span><span class="sh">"</span><span class="s">partition-id</span><span class="sh">"</span><span class="p">])</span>
    <span class="n">client_train</span> <span class="o">=</span> <span class="n">train_sets</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">partition_id</span><span class="p">)]</span> <span class="c1"># provide a training set to each client
</span>    <span class="n">client_test</span> <span class="o">=</span> <span class="n">testset</span>
    <span class="k">return</span> <span class="nc">FlowerClient</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">client_train</span><span class="p">,</span> <span class="n">client_test</span><span class="p">).</span><span class="nf">to_client</span><span class="p">()</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">ClientApp</span><span class="p">(</span><span class="n">client_fn</span><span class="p">)</span>
</code></pre></div></div> <ol> <li>Define an aggregation strategy on server side - here we use <code class="language-plaintext highlighter-rouge">FedAvg</code>.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="nc">SimpleModel</span><span class="p">()</span>
<span class="n">params</span> <span class="o">=</span> <span class="nf">ndarrays_to_parameters</span><span class="p">(</span><span class="nf">get_weights</span><span class="p">(</span><span class="n">net</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">server_fn</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="n">Context</span><span class="p">):</span>
    <span class="n">strategy</span> <span class="o">=</span> <span class="nc">FedAvg</span><span class="p">(</span>
        <span class="n">fraction_fit</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="c1"># fit on 100% of the clients
</span>        <span class="n">fraction_evaluate</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="c1"># evaluate on 0 clients
</span>        <span class="n">initial_parameters</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
        <span class="n">evaluate_fn</span><span class="o">=</span><span class="n">evaluate</span><span class="p">,</span> <span class="c1"># custom function to calculate accuracy after each epoch of training.
</span>    <span class="p">)</span>
    <span class="n">config</span><span class="o">=</span><span class="nc">ServerConfig</span><span class="p">(</span><span class="n">num_rounds</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># no. of communication rounds.
</span>    <span class="k">return</span> <span class="nc">ServerAppComponents</span><span class="p">(</span>
        <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># instantiate the server
</span><span class="n">server</span> <span class="o">=</span> <span class="nc">ServerApp</span><span class="p">(</span><span class="n">server_fn</span><span class="o">=</span><span class="n">server_fn</span><span class="p">)</span>
</code></pre></div></div> <ol> <li>Run a simulation for FL. <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Initiate the simulation passing the server and client apps
# Specify the number of super nodes that will be selected on every round
</span><span class="nf">run_simulation</span><span class="p">(</span>
 <span class="n">server_app</span><span class="o">=</span><span class="n">server</span><span class="p">,</span>
 <span class="n">client_app</span><span class="o">=</span><span class="n">client</span><span class="p">,</span>
 <span class="n">num_supernodes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
 <span class="n">backend_config</span><span class="o">=</span><span class="n">backend_setup</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div> </div> </li> </ol> <p><strong>Output:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO :      Starting Flower ServerApp, config: <span class="nv">num_rounds</span><span class="o">=</span>3, no round_timeout
INFO :      
INFO :      <span class="o">[</span>INIT]
INFO :      Using initial global parameters provided by strategy
INFO :      Evaluating initial global parameters
INFO :      <span class="nb">test </span>accuracy on all digits: 0.1267
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>1,3,7]: 0.2275
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>2,5,8]: 0.1201
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>4,6,9]: 0.0380
INFO :      
INFO :      <span class="o">[</span>ROUND 1]
INFO :      configure_fit: strategy sampled 3 clients <span class="o">(</span>out of 3<span class="o">)</span>
INFO :      aggregate_fit: received 3 results and 0 failures
INFO :      <span class="nb">test </span>accuracy on all digits: 0.8546
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>1,3,7]: 0.9477
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>2,5,8]: 0.7771
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>4,6,9]: 0.7823
INFO :      configure_evaluate: no clients selected, skipping evaluation
INFO :      
INFO :      <span class="o">[</span>ROUND 2]
INFO :      configure_fit: strategy sampled 3 clients <span class="o">(</span>out of 3<span class="o">)</span>
INFO :      aggregate_fit: received 3 results and 0 failures
INFO :      <span class="nb">test </span>accuracy on all digits: 0.9507
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>1,3,7]: 0.9619
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>2,5,8]: 0.9275
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>4,6,9]: 0.9464
INFO :      configure_evaluate: no clients selected, skipping evaluation
INFO :      
INFO :      <span class="o">[</span>ROUND 3]
INFO :      configure_fit: strategy sampled 3 clients <span class="o">(</span>out of 3<span class="o">)</span>
INFO :      aggregate_fit: received 3 results and 0 failures
INFO :      <span class="nb">test </span>accuracy on all digits: 0.9613
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>1,3,7]: 0.9672
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>2,5,8]: 0.9555
INFO :      <span class="nb">test </span>accuracy on <span class="o">[</span>4,6,9]: 0.9491
</code></pre></div></div> <blockquote> <p>We see that after 3 rounds of FL training, the accuracies of each model is close to 95% on the test set. This is a massive increase from 0% as seen before!**</p> </blockquote> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/final_model_cm-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/final_model_cm-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/final_model_cm-1400.webp"/> <img src="/assets/img/final_model_cm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="tuning">Tuning</h1> <p>There are multiple components in the FL process that can be tuned:</p> <ol> <li>Server side: <ul> <li>client selection: which clients will participate in each round</li> <li>client configuration: hyperparameters to use by each client for each round <ul> <li>hyperparameter tuning</li> </ul> </li> <li>result aggregation: methods to aggregate model weights on server side <ul> <li>QFedAvg</li> <li>FedAdam</li> <li>Vanilla FedAvg</li> <li>DP FedAvg</li> </ul> </li> </ul> </li> <li>Client-side: <ul> <li>preprocessing of weights before applying to the model</li> <li>local-training methodology</li> <li>post processing of weights before sending back to server</li> </ul> </li> </ol> <h1 id="privacy-in-federated-learning">Privacy in federated learning</h1> <p>FL is a data minimization solution. It prevents direct access to the data. However, an adversary can be present on any stage of an FL process.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/privacy-fl-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/privacy-fl-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/privacy-fl-1400.webp"/> <img src="/assets/img/privacy-fl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>These adversaries have the objective of leaking private data of one of the clients using targetted attacks.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fl-attacks-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fl-attacks-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fl-attacks-1400.webp"/> <img src="/assets/img/fl-attacks.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Therefore, many Privacy Enhancing Techniques (PETs) have been introduced to preserve the sensitive data of individuals from leaking in the case of an adversarial attack. One such is called Differential Privacy.</p> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/dp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/dp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/dp-1400.webp"/> <img src="/assets/img/dp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="differential-privacy-in-fl">Differential Privacy in FL</h2> <p>DP in FL aims to protect the privacy of client‚Äôs data. Depending in the intended level of privacy, utility, and the role of the adversary, it can be categorized into variants suh as central and local.</p> <p>This involves 2 steps:</p> <ol> <li> <p><strong>Clipping:</strong> Bounds the sensitivity and mitigates the impact of outliers.</p> </li> <li> <p><strong>Noising:</strong> Adds a calibrated noise to make the output statiscally indistinguishable.</p> </li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/local-dp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/local-dp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/local-dp-1400.webp"/> <img src="/assets/img/local-dp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/central-dp-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/central-dp-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/central-dp-1400.webp"/> <img src="/assets/img/central-dp.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Let‚Äôs implement it using flower.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flwr.server.strategy</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DifferentialPrivacyClientSideAdaptiveClipping</span><span class="p">,</span>
    <span class="n">FedAvg</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">server_fn</span><span class="p">(</span><span class="n">context</span><span class="p">:</span> <span class="n">Context</span><span class="p">):</span>
    <span class="n">fedavg_without_dp</span> <span class="o">=</span> <span class="nc">FedAvg</span><span class="p">(</span>
        <span class="n">fraction_fit</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
        <span class="n">fraction_evaluate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">initial_parameters</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Client side will clip the updates
</span>    <span class="n">fedavg_with_dp</span> <span class="o">=</span> <span class="nc">DifferentialPrivacyClientSideAdaptiveClipping</span><span class="p">(</span>
        <span class="n">fedavg_without_dp</span><span class="p">,</span>  <span class="c1"># &lt;- wrap the FedAvg strategy
</span>        <span class="n">noise_multiplier</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="c1"># Server will add noise after aggregation
</span>        <span class="n">num_sampled_clients</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="c1"># Adjust to 50 rounds to ensure DP guarantees hold
</span>    <span class="c1"># with respect to the desired privacy budget
</span>    <span class="n">config</span> <span class="o">=</span> <span class="nc">ServerConfig</span><span class="p">(</span><span class="n">num_rounds</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nc">ServerAppComponents</span><span class="p">(</span>
        <span class="n">strategy</span><span class="o">=</span><span class="n">fedavg_with_dp</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <hr/> <p>That is all for today folks! Blogging + learning is a long process haha, but I‚Äôm glad I learnt a bunch today!</p> <p>See you tomorrow :D</p> <h1 id="references">References</h1> <p>[1] <a href="https://learn.deeplearning.ai/courses/intro-to-federated-learning/">https://learn.deeplearning.ai/courses/intro-to-federated-learning/</a></p> <p>[2] <a href="https://www.v7labs.com/blog/federated-learning-guide">https://www.v7labs.com/blog/federated-learning-guide</a></p>]]></content><author><name></name></author><category term="open-source"/><category term="opensource"/><category term="ai"/><category term="federated-learning"/><category term="openmined"/><category term="challenge"/><category term="data-privacy"/><category term="trustworthy-ai"/><summary type="html"><![CDATA[It‚Äôs Day 1 of the #30DaysOfFLCode Challenge by OpenMined ü•≥!]]></summary></entry><entry><title type="html">Functions, Tools, and Agents</title><link href="https://orionstar25.github.io/blog/2024/function-calling/" rel="alternate" type="text/html" title="Functions, Tools, and Agents"/><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/function-calling</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/function-calling/"><![CDATA[<h1 id="function-calling">Function Calling</h1> <p>Function calling in large language models (LLMs) is a technique that allows LLMs to interact with external tools and APIs by:</p> <ol> <li> <p><strong>Detecting when a function is needed:</strong> LLMs are fine-tuned to identify when a function needs to be called based on a user‚Äôs prompt.</p> </li> <li> <p><strong>Generating a structured response:</strong> If applicable, LLMs generate a JSON object that contains the function name and arguments.</p> </li> <li> <p><strong>Executing the function:</strong> The developer‚Äôs code executes the function using the extracted arguments and to get an output.</p> </li> <li> <p><strong>Using the function output:</strong> The LLM uses the function output to generate a final response in natural language to the user.</p> </li> </ol> <p>In this post, I will explain the common structure of function-calling, tools, and agents using LangChain and OpenAI models.</p> <h2 id="langchain-expression-language-lcel">LangChain Expression Language (LCEL)</h2> <p>LangChain is an open-source framework that allows developers to build applications using large language models (LLMs). The LangChain Expression Language (LCEL) helps to compose a chain of LangChain components with minimal code.</p> <p>Typically, a chain contains the following components seperated by the <code class="language-plaintext highlighter-rouge">|</code> operator:</p> <blockquote> <table> <tbody> <tr> <td>Prompt</td> <td>LLM</td> <td>OutputParser</td> </tr> </tbody> </table> </blockquote> <p>Below is a code example to use LCEL and an LLM to generate a chain.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="n">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">langchain.schema.output_parser</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>

<span class="c1"># Create a prompt
</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">tell me a short joke about {topic}</span><span class="sh">"</span>
<span class="p">)</span>

<span class="c1"># Instantiate an LLM
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">()</span>

<span class="c1"># Instantiate a parser that converts model output to string
</span><span class="n">output_parser</span> <span class="o">=</span> <span class="nc">StrOutputParser</span><span class="p">()</span>

<span class="c1"># Create a langChain "chain" of components using LCEL
</span><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="n">output_parser</span>

<span class="c1"># Invoke the chain to generate a response from the LLM
</span><span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">topic</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">bears</span><span class="sh">"</span><span class="p">})</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">Why do bears have hairy coats?</span><span class="se">\n\n</span><span class="s">Because they don</span><span class="sh">'</span><span class="s">t like to shave!</span><span class="sh">"</span>
</code></pre></div></div> <ol> <li>The user creates a string prompt.</li> <li>This prompt is sent to the LLM to generate an output.</li> <li>The LLM output object is properly parsed to generate a string response for the user.</li> </ol> <h2 id="bind-custom-functions-to-llms">Bind custom functions to LLMs</h2> <p>The purpose of function-calling is:</p> <ol> <li>To leverage an LLM to analyse a a user query in order to determine whether a function-call is required to generate the desired response.</li> <li>If a function call is required, the LLM needs to figure out the correct function to call along with the required function argument values.</li> </ol> <p>This process helps the developer to then execute the relevant function with the arguments to generate the desired response. A function could be a custom function written by the developer or an API call that generates an output based on given arguments.</p> <p>OpenAI models expect the functions to be defined in the following format:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">functions</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span>
      <span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">function_name</span><span class="o">&gt;</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">function</span> <span class="n">description</span><span class="o">&gt;</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">parameters</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">object</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">properties</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
          <span class="o">&lt;</span><span class="n">argument_name</span><span class="o">&gt;</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">argument_type</span><span class="o">&gt;</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">argument_description</span><span class="o">&gt;</span>
          <span class="p">},</span>
        <span class="p">},</span>
        <span class="sh">"</span><span class="s">required</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="o">&lt;</span><span class="nb">list</span> <span class="n">of</span> <span class="n">required</span> <span class="n">arguments</span><span class="o">&gt;</span><span class="p">]</span>
      <span class="p">}</span>
   <span class="p">},</span>
<span class="bp">...</span>
<span class="p">]</span>
</code></pre></div></div> <p>Let‚Äôs take an example with 2 custom defined functions <code class="language-plaintext highlighter-rouge">weather_search</code> and <code class="language-plaintext highlighter-rouge">sports_search</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the functions
</span><span class="n">functions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">weather_search</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Search for weather given an airport code</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">parameters</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">object</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">properties</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
          <span class="sh">"</span><span class="s">airport_code</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">The airport code to get the weather for</span><span class="sh">"</span>
          <span class="p">},</span>
        <span class="p">},</span>
        <span class="sh">"</span><span class="s">required</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">airport_code</span><span class="sh">"</span><span class="p">]</span>
      <span class="p">}</span>
    <span class="p">},</span>
        <span class="p">{</span>
      <span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">sports_search</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Search for news of recent sport events</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">parameters</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">object</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">properties</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
          <span class="sh">"</span><span class="s">team_name</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">The sports team to search for</span><span class="sh">"</span>
          <span class="p">},</span>
        <span class="p">},</span>
        <span class="sh">"</span><span class="s">required</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">team_name</span><span class="sh">"</span><span class="p">]</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>

<span class="c1"># Create a prompt
</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{input}</span><span class="sh">"</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Instantiate an LLM and "bind" the defined functions with the LLM.
# This tells the LLM the list of available functions to choose from in case function-calling is required.
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bind</span><span class="p">(</span><span class="n">functions</span><span class="o">=</span><span class="n">functions</span><span class="p">)</span>

<span class="c1"># Create a chain
</span><span class="n">runnable</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span>
</code></pre></div></div> <p>Invoke the chain with a sports-related question:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runnable</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">how did the patriots do yesterday?</span><span class="sh">"</span><span class="p">})</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">additional_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">function_call</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">sports_search</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">arguments</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">{</span><span class="sh">"</span><span class="s">team_name</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">New England Patriots</span><span class="sh">"</span><span class="s">}</span><span class="sh">'</span><span class="p">}})</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">content</code> is empty because the LLM determined that a function needs to be called in order to generate a response.</li> <li><code class="language-plaintext highlighter-rouge">function_call</code> containts the following information: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">'</span><span class="s">function_call</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span>
 <span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">sports_search</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># the custom function that needs to be called 
</span> <span class="sh">'</span><span class="s">arguments</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">{</span><span class="sh">"</span><span class="s">team_name</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">New England Patriots</span><span class="sh">"</span><span class="s">}</span><span class="sh">'</span> <span class="c1"># the arguments to the function.
</span><span class="p">}</span>
</code></pre></div> </div> <p>The LLM automatically determined that <code class="language-plaintext highlighter-rouge">patriots</code> in the user query referred to the <code class="language-plaintext highlighter-rouge">New England Patriots</code> which is a sports team. Interesting!</p> </li> </ul> <p>Now, let‚Äôs invoke the chain with a weather related question:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runnable</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">what is the weather in sf</span><span class="sh">"</span><span class="p">})</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">additional_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">function_call</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">weather_search</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">arguments</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">{</span><span class="sh">"</span><span class="s">airport_code</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">SFO</span><span class="sh">"</span><span class="s">}</span><span class="sh">'</span><span class="p">}})</span>
</code></pre></div></div> <p>This time, the LLM determined that the function <code class="language-plaintext highlighter-rouge">weather_search</code> needs to be called with the argument <code class="language-plaintext highlighter-rouge">airport_code=SFO</code> where <code class="language-plaintext highlighter-rouge">SFO</code> stands for San Francisco!</p> <p>By default, if none of the defined functions are relevant to the user query, the LLM will not invoke function-calling and generate a normal user (natural language) response. It doesn‚Äôt forcibly use any function or hallucinate its arguments - until explicitely made to do so.</p> <h2 id="use-pydantic-to-create-functions-with-ease">Use pydantic to create functions with ease</h2> <p>Pydantic is a Python library for data validation using Python-type annotations. It ensures that the data you work with matches your specified data types, simplifying error handling and data parsing in Python applications.</p> <p>Thus, Pydantic will help us define LLM functions with ease. Let‚Äôs look at an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define a Pydantic class which refers to a function
# The fields of the class refer to function arguments
# The docstring used to describe the class is mandatory as it helps the LLM understand what the purpose of the function is. 
</span>
<span class="k">class</span> <span class="nc">WeatherSearch</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Call this with an airport code to get the weather at that airport</span><span class="sh">"""</span>
    <span class="n">airport_code</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">airport code to get weather for</span><span class="sh">"</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain.utils.openai_functions</span> <span class="kn">import</span> <span class="n">convert_pydantic_to_openai_function</span>
<span class="nf">convert_pydantic_to_openai_function</span><span class="p">(</span><span class="n">WeatherSearch</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">WeatherSearch</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># name of the class
</span> <span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Call this with an airport code to get the weather at that airport</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># Pulled from the docstring used to describe the class.
</span> <span class="sh">'</span><span class="s">parameters</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">WeatherSearch</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Call this with an airport code to get the weather at that airport</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">,</span>
  <span class="sh">'</span><span class="s">properties</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">airport_code</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Airport Code</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">airport code to get weather for</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">string</span><span class="sh">'</span><span class="p">}},</span>
  <span class="sh">'</span><span class="s">required</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">airport_code</span><span class="sh">'</span><span class="p">]}}</span>
</code></pre></div></div> <p>Let‚Äôs create multiple functions using Pydantic!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Custom function with 2 args
</span><span class="k">class</span> <span class="nc">ArtistSearch</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Call this to get the names of songs by a particular artist</span><span class="sh">"""</span>
    <span class="n">artist_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">name of artist to look up</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">number of results</span><span class="sh">"</span><span class="p">)</span>

<span class="n">functions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">convert_pydantic_to_openai_function</span><span class="p">(</span><span class="n">WeatherSearch</span><span class="p">),</span>
    <span class="nf">convert_pydantic_to_openai_function</span><span class="p">(</span><span class="n">ArtistSearch</span><span class="p">),</span>
<span class="p">]</span>

<span class="c1"># Bind the functions to the LLM
</span><span class="n">model_with_functions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">bind</span><span class="p">(</span><span class="n">functions</span><span class="o">=</span><span class="n">functions</span><span class="p">)</span>
</code></pre></div></div> <p>Invoking with weather related question:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_with_functions</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">what is the weather in sf?</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">additional_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">function_call</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">WeatherSearch</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">arguments</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">{</span><span class="sh">"</span><span class="s">airport_code</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">SFO</span><span class="sh">"</span><span class="s">}</span><span class="sh">'</span><span class="p">}})</span>
</code></pre></div></div> <p>Invoking with artist related question:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_with_functions</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">what are three songs by taylor swift?</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">additional_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">function_call</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">ArtistSearch</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">arguments</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">{</span><span class="sh">"</span><span class="s">artist_name</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="s">Taylor Swift</span><span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="s">n</span><span class="sh">"</span><span class="s">:3}</span><span class="sh">'</span><span class="p">}})</span>
</code></pre></div></div> <p>Default behaviour:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_with_functions</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="sh">"</span><span class="s">hi!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">Hello! How can I assist you today?</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h2 id="extraction">Extraction</h2> <p>Concept extraction is an NLP task that automatically identifies and extracts specific concepts or entities from unstructured text.</p> <p>Let‚Äôs see how function-calling can help us solve this task of extracting all the papers and their respective authors mentioned in an unstructured article with ease.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load unstructured text from the internet
</span><span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="nc">WebBaseLoader</span><span class="p">(</span><span class="sh">"</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">"</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
<span class="n">page_content</span> <span class="o">=</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">page_content</span><span class="p">[:</span><span class="mi">10000</span><span class="p">]</span>

<span class="c1"># Create a prompt
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">A article will be passed to you. Extract from it all papers that are mentioned by this article followed by its author. 

Do not extract the name of the article itself. If no papers are mentioned that</span><span class="sh">'</span><span class="s">s fine - you don</span><span class="sh">'</span><span class="s">t need to extract any! Just return an empty list.

Do not make up or guess ANY extra information. Only extract what exactly is in the text.</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="n">template</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{input}</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Create custom function definitions
</span><span class="k">class</span> <span class="nc">Paper</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Information about papers mentioned.</span><span class="sh">"""</span>
    <span class="n">title</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">author</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">Info</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Information to extract</span><span class="sh">"""</span>
    <span class="n">papers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Paper</span><span class="p">]</span>

<span class="n">paper_extraction_function</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nf">convert_pydantic_to_openai_function</span><span class="p">(</span><span class="n">Info</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Bind the function to the LLM
</span><span class="n">extraction_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">bind</span><span class="p">(</span>
    <span class="n">functions</span><span class="o">=</span><span class="n">paper_extraction_function</span><span class="p">,</span> <span class="c1"># functions present for binding
</span>    <span class="n">function_call</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">Info</span><span class="sh">"</span><span class="p">}</span> <span class="c1"># Force the LLM to use the function "Info" everytime
</span><span class="p">)</span>

<span class="c1"># Create a chain and invoke it with the article
</span><span class="n">extraction_chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">extraction_model</span> <span class="o">|</span> <span class="nc">JsonKeyOutputFunctionsParser</span><span class="p">(</span><span class="n">key_name</span><span class="o">=</span><span class="sh">"</span><span class="s">papers</span><span class="sh">"</span><span class="p">)</span>
<span class="n">extraction_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">page_content</span><span class="p">})</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Chain of thought (CoT; Wei et al. 2022)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Wei et al. 2022</span><span class="sh">'</span><span class="p">},</span>
 <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Tree of Thoughts (Yao et al. 2023)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Yao et al. 2023</span><span class="sh">'</span><span class="p">},</span>
 <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">LLM+P (Liu et al. 2023)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Liu et al. 2023</span><span class="sh">'</span><span class="p">},</span>
 <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">ReAct (Yao et al. 2023)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Yao et al. 2023</span><span class="sh">'</span><span class="p">},</span>
 <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Reflexion (Shinn &amp; Labash 2023)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Shinn &amp; Labash 2023</span><span class="sh">'</span><span class="p">},</span>
 <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Chain of Hindsight (CoH; Liu et al. 2023)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Liu et al. 2023</span><span class="sh">'</span><span class="p">},</span>
 <span class="p">{</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Algorithm Distillation (AD; Laskin et al. 2023)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">author</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Laskin et al. 2023</span><span class="sh">'</span><span class="p">}]</span>
</code></pre></div></div> <p>The chain has successfully extracted all the papers that were mentioned in the article along with its authors in a structured JSON format.</p> <p>That‚Äôs all for function-calling (for now)!</p> <h1 id="tools-and-routing">Tools and Routing</h1> <ol> <li> <p><strong>Tools:</strong> Functions and services an LLM can utilize to extend its capabilities are named ‚Äútools‚Äù in LangChain.</p> </li> <li> <p><strong>Routing:</strong> Selecting from multiple possible tools is called ‚Äúrouting‚Äù.</p> </li> </ol> <h2 id="tools">Tools</h2> <p>Let‚Äôs create a function <code class="language-plaintext highlighter-rouge">get_current_temperature()</code> that calls an external weather API to get the temperature forecast for the next day.</p> <p>Each function can be converted into a tool using the <code class="language-plaintext highlighter-rouge">@tool</code> decorator. <code class="language-plaintext highlighter-rouge">args_schema</code> specifies the format of the args for this function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.agents</span> <span class="kn">import</span> <span class="n">tool</span>

<span class="c1"># Define the input schema
</span><span class="k">class</span> <span class="nc">OpenMeteoInput</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">latitude</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(...,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Latitude of the location to fetch weather data for</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">longitude</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(...,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Longitude of the location to fetch weather data for</span><span class="sh">"</span><span class="p">)</span>

<span class="nd">@tool</span><span class="p">(</span><span class="n">args_schema</span><span class="o">=</span><span class="n">OpenMeteoInput</span><span class="p">)</span> <span class="c1"># args_schema tells the format of the args for this function
</span><span class="k">def</span> <span class="nf">get_current_temperature</span><span class="p">(</span><span class="n">latitude</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">longitude</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Fetch current temperature for given coordinates.</span><span class="sh">"""</span>
    
    <span class="n">BASE_URL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">https://api.open-meteo.com/v1/forecast</span><span class="sh">"</span>
    
    <span class="c1"># Parameters for the request
</span>    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">latitude</span><span class="sh">'</span><span class="p">:</span> <span class="n">latitude</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">longitude</span><span class="sh">'</span><span class="p">:</span> <span class="n">longitude</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">hourly</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">temperature_2m</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">forecast_days</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="c1"># Make the request
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">BASE_URL</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">Exception</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">API Request failed with status code: </span><span class="si">{</span><span class="n">response</span><span class="p">.</span><span class="n">status_code</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">current_utc_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">utcnow</span><span class="p">()</span>
    <span class="n">time_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="nf">fromisoformat</span><span class="p">(</span><span class="n">time_str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">Z</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">+00:00</span><span class="sh">'</span><span class="p">))</span> <span class="k">for</span> <span class="n">time_str</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">hourly</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">time</span><span class="sh">'</span><span class="p">]]</span>
    <span class="n">temperature_list</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">hourly</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">temperature_2m</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="n">closest_time_index</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">time_list</span><span class="p">)),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="nf">abs</span><span class="p">(</span><span class="n">time_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">current_utc_time</span><span class="p">))</span>
    <span class="n">current_temperature</span> <span class="o">=</span> <span class="n">temperature_list</span><span class="p">[</span><span class="n">closest_time_index</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="sa">f</span><span class="sh">'</span><span class="s">The current temperature is </span><span class="si">{</span><span class="n">current_temperature</span><span class="si">}</span><span class="s">¬∞C</span><span class="sh">'</span>
</code></pre></div></div> <p>Every tool has the following properties:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">get_current_temperature</span><span class="p">.</span><span class="n">name</span>
<span class="o">&gt;</span> <span class="sh">'</span><span class="s">get_current_temperature</span><span class="sh">'</span>

<span class="n">get_current_temperature</span><span class="p">.</span><span class="n">description</span>
<span class="o">&gt;</span> <span class="sh">'</span><span class="s">get_current_temperature(latitude: float, longitude: float) -&gt; dict - Fetch current temperature for given coordinates.</span><span class="sh">'</span>

<span class="c1"># followed the defined schema
</span><span class="n">get_current_temperature</span><span class="p">.</span><span class="n">args</span>
<span class="o">&gt;</span> <span class="sh">"</span><span class="s">{</span><span class="sh">'</span><span class="s">latitude</span><span class="sh">'</span><span class="s">: {</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="s">,
  </span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Latitude of the location to fetch weather data for</span><span class="sh">'</span><span class="s">,
  </span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">number</span><span class="sh">'</span><span class="s">},
 </span><span class="sh">'</span><span class="s">longitude</span><span class="sh">'</span><span class="s">: {</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="s">,
  </span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Longitude of the location to fetch weather data for</span><span class="sh">'</span><span class="s">,
  </span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">number</span><span class="sh">'</span><span class="s">}}</span><span class="sh">"</span>
</code></pre></div></div> <p>You can convert any tool into an openai function format.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.tools.render</span> <span class="kn">import</span> <span class="n">format_tool_to_openai_function</span>

<span class="nf">format_tool_to_openai_function</span><span class="p">(</span><span class="n">get_current_temperature</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="sh">"</span><span class="s">{</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">get_current_temperature</span><span class="sh">'</span><span class="s">,
 </span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">get_current_temperature(latitude: float, longitude: float) -&gt; dict - Fetch current temperature for given coordinates.</span><span class="sh">'</span><span class="s">,
 </span><span class="sh">'</span><span class="s">parameters</span><span class="sh">'</span><span class="s">: {</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">OpenMeteoInput</span><span class="sh">'</span><span class="s">,
  </span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="s">,
  </span><span class="sh">'</span><span class="s">properties</span><span class="sh">'</span><span class="s">: {</span><span class="sh">'</span><span class="s">latitude</span><span class="sh">'</span><span class="s">: {</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="s">,
    </span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Latitude of the location to fetch weather data for</span><span class="sh">'</span><span class="s">,
    </span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">number</span><span class="sh">'</span><span class="s">},
   </span><span class="sh">'</span><span class="s">longitude</span><span class="sh">'</span><span class="s">: {</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="s">,
    </span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">Longitude of the location to fetch weather data for</span><span class="sh">'</span><span class="s">,
    </span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="s">: </span><span class="sh">'</span><span class="s">number</span><span class="sh">'</span><span class="s">}},
  </span><span class="sh">'</span><span class="s">required</span><span class="sh">'</span><span class="s">: [</span><span class="sh">'</span><span class="s">latitude</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">longitude</span><span class="sh">'</span><span class="s">]}}</span><span class="sh">"</span>
</code></pre></div></div> <h2 id="routing">Routing</h2> <p>Let‚Äôs create another tool <code class="language-plaintext highlighter-rouge">search_wikipedia()</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">wikipedia</span>
<span class="nd">@tool</span>
<span class="k">def</span> <span class="nf">search_wikipedia</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Run Wikipedia search and get page summaries.</span><span class="sh">"""</span>
    <span class="n">page_titles</span> <span class="o">=</span> <span class="n">wikipedia</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">summaries</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">page_title</span> <span class="ow">in</span> <span class="n">page_titles</span><span class="p">[:</span> <span class="mi">3</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">wiki_page</span> <span class="o">=</span>  <span class="n">wikipedia</span><span class="p">.</span><span class="nf">page</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">page_title</span><span class="p">,</span> <span class="n">auto_suggest</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">summaries</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Page: </span><span class="si">{</span><span class="n">page_title</span><span class="si">}</span><span class="se">\n</span><span class="s">Summary: </span><span class="si">{</span><span class="n">wiki_page</span><span class="p">.</span><span class="n">summary</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">except </span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">wiki_client</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">PageError</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">wiki_client</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">DisambiguationError</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">pass</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">summaries</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">No good Wikipedia Search Result was found</span><span class="sh">"</span>
    <span class="k">return</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span>
</code></pre></div></div> <p>The response of running any tool can be either:</p> <ol> <li><strong>AgentAction</strong>: specifies what the next action should be in order to get the final response, i.e., call a specific function using arguments after analysing the user input query.</li> <li><strong>AgentFinish</strong>: provides final user content/response.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.schema.agent</span> <span class="kn">import</span> <span class="n">AgentFinish</span>

<span class="c1"># create a routing 
</span><span class="k">def</span> <span class="nf">route</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
    <span class="c1"># If the result is AgentFinish, no more action is needed.
</span>    <span class="c1"># return the final output to the user
</span>    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">AgentFinish</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="n">return_values</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tools</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">search_wikipedia</span><span class="sh">"</span><span class="p">:</span> <span class="n">search_wikipedia</span><span class="p">,</span> 
            <span class="sh">"</span><span class="s">get_current_temperature</span><span class="sh">"</span><span class="p">:</span> <span class="n">get_current_temperature</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># call the function identified by the LLM with the corresponding args
</span>        <span class="k">return</span> <span class="n">tools</span><span class="p">[</span><span class="n">result</span><span class="p">.</span><span class="n">tool</span><span class="p">].</span><span class="nf">run</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">tool_input</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create a chain that 
</span>  <span class="c1"># takes a prompt, 
</span>  <span class="c1"># identifies if function calling is required, 
</span>  <span class="c1"># parses the output, 
</span>  <span class="c1"># routes to the required function-call, executes the function to retrieve the final response.
</span><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="nc">OpenAIFunctionsAgentOutputParser</span><span class="p">()</span> <span class="o">|</span> <span class="n">route</span>

<span class="c1"># This used AgentAction to call `get_current_weather()`
</span><span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is the weather in san francisco right now?</span><span class="sh">"</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="sh">'</span><span class="s">The current temperature is 12.1¬∞C</span><span class="sh">'</span>

<span class="c1"># This used AgentFinish since no function-call was required
</span><span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">hi!</span><span class="sh">"</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="sh">'</span><span class="s">Hello! How can I assist you today?</span><span class="sh">'</span>
</code></pre></div></div> <h1 id="conversational-agents">Conversational Agents</h1> <h2 id="what-are-agents">What are agents?</h2> <p>A combination of LLMs and code. LLMs reason about what steps to take and call for actions.</p> <h2 id="agent-loop">Agent Loop</h2> <ol> <li>Choose a tool to use.</li> <li>Observe the output of the tool.</li> <li>Repeat until a stopping condition is met. <ul> <li>LLM determined.</li> <li>Hardcorded rules.</li> </ul> </li> </ol> <p>Give the LLM all intermediate results and their observations for the LLM to identify the next steps. This is done by populating intermediate results in <code class="language-plaintext highlighter-rouge">agent_scratchpad</code> inside the prompt everytime the LLM is called.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.schema.runnable</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="nc">OpenAIFunctionsAgentOutputParser</span><span class="p">()</span>
<span class="c1"># RunnablePassthrough helps in passing intermediate steps in the prompt everytime the LLM is called
</span><span class="n">agent_chain</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span>
    <span class="n">agent_scratchpad</span><span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">format_to_openai_functions</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">intermediate_steps</span><span class="sh">"</span><span class="p">])</span>
<span class="p">)</span> <span class="o">|</span> <span class="n">chain</span>

<span class="k">def</span> <span class="nf">run_agent</span><span class="p">(</span><span class="n">user_input</span><span class="p">):</span>
    <span class="n">intermediate_steps</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">agent_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">,</span> 
            <span class="sh">"</span><span class="s">intermediate_steps</span><span class="sh">"</span><span class="p">:</span> <span class="n">intermediate_steps</span>
        <span class="p">})</span>

        <span class="c1"># if final output is received, return 
</span>        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">AgentFinish</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">result</span>

        <span class="n">tool</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">search_wikipedia</span><span class="sh">"</span><span class="p">:</span> <span class="n">search_wikipedia</span><span class="p">,</span> 
            <span class="sh">"</span><span class="s">get_current_temperature</span><span class="sh">"</span><span class="p">:</span> <span class="n">get_current_temperature</span><span class="p">,</span>
        <span class="p">}[</span><span class="n">result</span><span class="p">.</span><span class="n">tool</span><span class="p">]</span>
        
        <span class="c1"># get the observation from the respective tool
</span>        <span class="n">observation</span> <span class="o">=</span> <span class="n">tool</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">tool_input</span><span class="p">)</span>

        <span class="c1"># populate the tool used and its observation in the prompt for next iteration
</span>        <span class="n">intermediate_steps</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">result</span><span class="p">,</span> <span class="n">observation</span><span class="p">))</span>


<span class="nf">run_agent</span><span class="p">(</span><span class="sh">"</span><span class="s">what is the weather is sf?</span><span class="sh">"</span><span class="p">)</span>

<span class="o">&gt;</span> <span class="nc">AgentFinish</span><span class="p">(</span><span class="n">return_values</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">The current temperature in San Francisco is 11.8¬∞C.</span><span class="sh">'</span><span class="p">},</span> <span class="n">log</span><span class="o">=</span><span class="sh">'</span><span class="s">The current temperature in San Francisco is 11.8¬∞C.</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">agent_executor</code> is a helpful abstraction to create e-2-e agents.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.agents</span> <span class="kn">import</span> <span class="n">AgentExecutor</span>

<span class="n">agent_executor</span> <span class="o">=</span> <span class="nc">AgentExecutor</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent_chain</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">agent_executor</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">what is langchain?</span><span class="sh">"</span><span class="p">})</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Entering new AgentExecutor chain...

Invoking: `search_wikipedia` with `{'query': 'Langchain'}`

Page: LangChain
Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework
...
&gt; Finished chain.

{'input': 'what is langchain?',
 'output': 'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for document analysis and summarization, chatbots, and code analysis.'}
</code></pre></div></div> <h2 id="chat-history">Chat History</h2> <p>By default, LLMs don‚Äôt have history of previous interactions. We can add the chat history using <code class="language-plaintext highlighter-rouge">ConversationBufferMemory</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.memory</span> <span class="kn">import</span> <span class="n">ConversationBufferMemory</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">You are helpful but sassy assistant</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">),</span> <span class="c1"># populate the chat history inside prompt here
</span>    <span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{input}</span><span class="sh">"</span><span class="p">),</span>
    <span class="nc">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="sh">"</span><span class="s">agent_scratchpad</span><span class="sh">"</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># keep the history inside memory buffer
</span><span class="n">memory</span> <span class="o">=</span> <span class="nc">ConversationBufferMemory</span><span class="p">(</span><span class="n">return_messages</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">)</span>

<span class="n">agent_executor</span> <span class="o">=</span> <span class="nc">AgentExecutor</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent_chain</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">agent_executor</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">my name is bob</span><span class="sh">"</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Entering</span> <span class="n">new</span> <span class="n">AgentExecutor</span> <span class="n">chain</span><span class="bp">...</span>
<span class="n">Hello</span> <span class="n">Bob</span><span class="err">!</span> <span class="n">How</span> <span class="n">can</span> <span class="n">I</span> <span class="n">assist</span> <span class="n">you</span> <span class="n">today</span><span class="err">?</span>

<span class="o">&gt;</span> <span class="n">Finished</span> <span class="n">chain</span><span class="p">.</span>
<span class="p">{</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">my name is bob</span><span class="sh">'</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">chat_history</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="nc">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">my name is bob</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">Hello Bob! How can I assist you today?</span><span class="sh">'</span><span class="p">)],</span>
 <span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Hello Bob! How can I assist you today?</span><span class="sh">'</span><span class="p">}</span>

<span class="o">----------</span>

<span class="n">agent_executor</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">whats my name</span><span class="sh">"</span><span class="p">})</span>

<span class="o">&gt;</span> <span class="n">Entering</span> <span class="n">new</span> <span class="n">AgentExecutor</span> <span class="n">chain</span><span class="bp">...</span>
<span class="n">Your</span> <span class="n">name</span> <span class="ow">is</span> <span class="n">Bob</span><span class="p">.</span> <span class="n">How</span> <span class="n">can</span> <span class="n">I</span> <span class="n">assist</span> <span class="n">you</span> <span class="n">today</span><span class="p">,</span> <span class="n">Bob</span><span class="err">?</span>

<span class="o">&gt;</span> <span class="n">Finished</span> <span class="n">chain</span><span class="p">.</span>
<span class="p">{</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">whats my name</span><span class="sh">'</span><span class="p">,</span>
 <span class="sh">'</span><span class="s">chat_history</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="nc">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">my name is bob</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">Hello Bob! How can I assist you today?</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">whats my name</span><span class="sh">'</span><span class="p">),</span>
  <span class="nc">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="sh">'</span><span class="s">Your name is Bob. How can I assist you today, Bob?</span><span class="sh">'</span><span class="p">)],</span>
 <span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Your name is Bob. How can I assist you today, Bob?</span><span class="sh">'</span><span class="p">}</span>
</code></pre></div></div> <h2 id="chatbot">Chatbot</h2> <p>Let‚Äôs make a conversational bot using everything we learnt!! Also, I am on a time-constraint right now, that‚Äôs why I‚Äôm just re-iterating what the tutorial taught. But someday (when I have enough time and I‚Äôm feeling creative), I‚Äôll create a fun chatbot of my own.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">panel</span> <span class="k">as</span> <span class="n">pn</span>  <span class="c1"># GUI
</span><span class="n">pn</span><span class="p">.</span><span class="nf">extension</span><span class="p">()</span>
<span class="kn">import</span> <span class="n">panel</span> <span class="k">as</span> <span class="n">pn</span>
<span class="kn">import</span> <span class="n">param</span>


<span class="c1"># A helper class to conbine all concepts learnt above
</span><span class="k">class</span> <span class="nc">cbfs</span><span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="n">Parameterized</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tools</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">cbfs</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">(</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">panels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">functions</span> <span class="o">=</span> <span class="p">[</span><span class="nf">format_tool_to_openai_function</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">bind</span><span class="p">(</span><span class="n">functions</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">functions</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nc">ConversationBufferMemory</span><span class="p">(</span><span class="n">return_messages</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">memory_key</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">([</span>
            <span class="p">(</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">You are helpful but sassy assistant</span><span class="sh">"</span><span class="p">),</span>
            <span class="nc">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="sh">"</span><span class="s">chat_history</span><span class="sh">"</span><span class="p">),</span>
            <span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{input}</span><span class="sh">"</span><span class="p">),</span>
            <span class="nc">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="sh">"</span><span class="s">agent_scratchpad</span><span class="sh">"</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">chain</span> <span class="o">=</span> <span class="n">RunnablePassthrough</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span>
            <span class="n">agent_scratchpad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">format_to_openai_functions</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">intermediate_steps</span><span class="sh">"</span><span class="p">])</span>
        <span class="p">)</span> <span class="o">|</span> <span class="n">self</span><span class="p">.</span><span class="n">prompt</span> <span class="o">|</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">|</span> <span class="nc">OpenAIFunctionsAgentOutputParser</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">qa</span> <span class="o">=</span> <span class="nc">AgentExecutor</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">chain</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">memory</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">convchain</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">query</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">inp</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="sh">''</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">qa</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>
        <span class="n">self</span><span class="p">.</span><span class="n">answer</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">]</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">panels</span><span class="p">.</span><span class="nf">extend</span><span class="p">([</span>
            <span class="n">pn</span><span class="p">.</span><span class="nc">Row</span><span class="p">(</span><span class="sh">'</span><span class="s">User:</span><span class="sh">'</span><span class="p">,</span> <span class="n">pn</span><span class="p">.</span><span class="n">pane</span><span class="p">.</span><span class="nc">Markdown</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">450</span><span class="p">)),</span>
            <span class="n">pn</span><span class="p">.</span><span class="nc">Row</span><span class="p">(</span><span class="sh">'</span><span class="s">ChatBot:</span><span class="sh">'</span><span class="p">,</span> <span class="n">pn</span><span class="p">.</span><span class="n">pane</span><span class="p">.</span><span class="nc">Markdown</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">answer</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">450</span><span class="p">,</span> <span class="n">styles</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">background-color</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">#F6F6F6</span><span class="sh">'</span><span class="p">}))</span>
        <span class="p">])</span>
        <span class="k">return</span> <span class="n">pn</span><span class="p">.</span><span class="nc">WidgetBox</span><span class="p">(</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">panels</span><span class="p">,</span> <span class="n">scroll</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">clr_history</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">count</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">chat_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> 
</code></pre></div></div> <p>Putting it all together in a nice simple UI.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cb</span> <span class="o">=</span> <span class="nf">cbfs</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>
<span class="n">inp</span> <span class="o">=</span> <span class="n">pn</span><span class="p">.</span><span class="n">widgets</span><span class="p">.</span><span class="nc">TextInput</span><span class="p">(</span> <span class="n">placeholder</span><span class="o">=</span><span class="sh">'</span><span class="s">Enter text here‚Ä¶</span><span class="sh">'</span><span class="p">)</span>
<span class="n">conversation</span> <span class="o">=</span> <span class="n">pn</span><span class="p">.</span><span class="nf">bind</span><span class="p">(</span><span class="n">cb</span><span class="p">.</span><span class="n">convchain</span><span class="p">,</span> <span class="n">inp</span><span class="p">)</span> 

<span class="n">tab1</span> <span class="o">=</span> <span class="n">pn</span><span class="p">.</span><span class="nc">Column</span><span class="p">(</span>
    <span class="n">pn</span><span class="p">.</span><span class="nc">Row</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span>
    <span class="n">pn</span><span class="p">.</span><span class="n">layout</span><span class="p">.</span><span class="nc">Divider</span><span class="p">(),</span>
    <span class="n">pn</span><span class="p">.</span><span class="nf">panel</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span>  <span class="n">loading_indicator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">),</span>
    <span class="n">pn</span><span class="p">.</span><span class="n">layout</span><span class="p">.</span><span class="nc">Divider</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">dashboard</span> <span class="o">=</span> <span class="n">pn</span><span class="p">.</span><span class="nc">Column</span><span class="p">(</span>
    <span class="n">pn</span><span class="p">.</span><span class="nc">Row</span><span class="p">(</span><span class="n">pn</span><span class="p">.</span><span class="n">pane</span><span class="p">.</span><span class="nc">Markdown</span><span class="p">(</span><span class="sh">'</span><span class="s"># QnA_Bot</span><span class="sh">'</span><span class="p">)),</span>
    <span class="n">pn</span><span class="p">.</span><span class="nc">Tabs</span><span class="p">((</span><span class="sh">'</span><span class="s">Conversation</span><span class="sh">'</span><span class="p">,</span> <span class="n">tab1</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">dashboard</span>
</code></pre></div></div> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/chatbot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/chatbot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/chatbot-1400.webp"/> <img src="/assets/img/chatbot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p>That‚Äôs all folks! I have a few other agent concepts to learn in the pipeline. Let the learning continue üòÑ.</p>]]></content><author><name></name></author><category term="open-source"/><category term="opensource"/><category term="ai"/><category term="github"/><summary type="html"><![CDATA[Understand the capabilities of LLMs to the fullest.]]></summary></entry><entry><title type="html">A list of Open Source AI Projects</title><link href="https://orionstar25.github.io/blog/2024/oss-ai-projects/" rel="alternate" type="text/html" title="A list of Open Source AI Projects"/><published>2024-11-01T00:00:00+00:00</published><updated>2024-11-01T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/oss-ai-projects</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/oss-ai-projects/"><![CDATA[<p>Contributing to open-source AI projects is a fantastic idea, especially for developing skills, showcasing your work, and networking with the broader community. There are several beginner-friendly yet impactful projects you can explore. Here are some to consider:</p> <blockquote> <p>NOTE: This list has been generated using ChatGPT, October 2024</p> </blockquote> <h3 id="1-hugging-face--transformers">1. <strong>Hugging Face (ü§ó Transformers)</strong></h3> <ul> <li><strong>Overview</strong>: Hugging Face is one of the leading platforms in NLP and AI. The <em>Transformers</em> library is widely used for NLP models and tasks.</li> <li><strong>Why it‚Äôs great</strong>: There‚Äôs a supportive community, excellent documentation, and many open issues. Beginners can contribute by improving models, datasets, documentation, or tutorials.</li> <li><strong>Skills you‚Äôll gain</strong>: NLP, model fine-tuning, understanding transformer architectures.</li> <li><strong>Get started</strong>: Check out their <a href="https://github.com/huggingface/transformers/issues">open issues</a> and community projects. You can contribute by helping with tutorials or working on model integration.</li> </ul> <h3 id="2-tensorflow">2. <strong>TensorFlow</strong></h3> <ul> <li><strong>Overview</strong>: TensorFlow, backed by Google, is one of the most widely used ML frameworks. It offers projects ranging from beginners to advanced levels.</li> <li><strong>Why it‚Äôs great</strong>: There‚Äôs a large, active community. Even small contributions can have a big impact. TensorFlow has a wide array of subprojects (e.g., TensorFlow Lite for mobile and TensorFlow.js for JavaScript).</li> <li><strong>Skills you‚Äôll gain</strong>: Deep learning, TensorFlow architecture, model building and deployment.</li> <li><strong>Get started</strong>: Visit the <a href="https://github.com/tensorflow/tensorflow/issues">TensorFlow issues page</a> to find areas where you can contribute, from fixing bugs to writing examples and tutorials.</li> </ul> <h3 id="3-fastai">3. <strong>FastAI</strong></h3> <ul> <li><strong>Overview</strong>: FastAI is an easy-to-use deep learning library built on top of PyTorch. It‚Äôs designed to be accessible for learners and offers top-level results with minimal coding.</li> <li><strong>Why it‚Äôs great</strong>: FastAI has a very friendly and inclusive community. Contributing to documentation, tutorials, or even model improvements are all welcome.</li> <li><strong>Skills you‚Äôll gain</strong>: PyTorch, deep learning, computer vision, and NLP.</li> <li><strong>Get started</strong>: Check out their <a href="https://github.com/fastai/fastai">GitHub repository</a> for beginner-friendly issues and join their forums to engage with other contributors.</li> </ul> <h3 id="4-pytorch">4. <strong>PyTorch</strong></h3> <ul> <li><strong>Overview</strong>: PyTorch is another popular open-source deep learning library. Backed by Facebook AI, it‚Äôs known for its flexibility and ease of use.</li> <li><strong>Why it‚Äôs great</strong>: Contributing to PyTorch can help you understand deep learning frameworks in depth. You can contribute to various areas like improving models, adding new features, or enhancing performance.</li> <li><strong>Skills you‚Äôll gain</strong>: PyTorch, deep learning, understanding hardware acceleration (if you dive into GPU optimizations).</li> <li><strong>Get started</strong>: PyTorch has <a href="https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first issues</a> that are ideal for beginners.</li> </ul> <h3 id="5-openmined">5. <strong>OpenMined</strong></h3> <ul> <li><strong>Overview</strong>: OpenMined is an open-source community focused on privacy-preserving AI. Their flagship projects like PySyft allow training ML models on private data without accessing the data directly.</li> <li><strong>Why it‚Äôs great</strong>: You‚Äôll get exposure to cutting-edge techniques in privacy, such as federated learning and differential privacy.</li> <li><strong>Skills you‚Äôll gain</strong>: Privacy-preserving machine learning, PyTorch, federated learning.</li> <li><strong>Get started</strong>: Visit their <a href="https://github.com/OpenMined">GitHub</a> and check out the community discussions and first issues to contribute.</li> </ul> <h3 id="6-deepchem">6. <strong>DeepChem</strong></h3> <ul> <li><strong>Overview</strong>: DeepChem is a library focused on democratizing deep learning for science, particularly in drug discovery and materials science.</li> <li><strong>Why it‚Äôs great</strong>: If you‚Äôre interested in the intersection of AI and life sciences, this is a great place to start. Contributions range from improving documentation to developing models for new scientific datasets.</li> <li><strong>Skills you‚Äôll gain</strong>: AI in life sciences, PyTorch/TensorFlow, model building and application.</li> <li><strong>Get started</strong>: Explore their <a href="https://github.com/deepchem/deepchem">contribution guide</a> and look for issues tagged as beginner-friendly.</li> </ul> <h3 id="7-ml5js">7. <strong>ML5.js</strong></h3> <ul> <li><strong>Overview</strong>: ML5.js is a JavaScript library that makes machine learning accessible to the web developer community. It wraps TensorFlow.js and allows ML models to run in-browser.</li> <li><strong>Why it‚Äôs great</strong>: It‚Äôs very beginner-friendly, especially if you have some JavaScript knowledge. It‚Äôs also fun to build and deploy AI-powered web apps.</li> <li><strong>Skills you‚Äôll gain</strong>: Web-based AI, TensorFlow.js, interactive app development.</li> <li><strong>Get started</strong>: Check out the <a href="https://github.com/ml5js/ml5-library/issues">ML5.js GitHub issues</a> for contribution ideas.</li> </ul> <h3 id="8-scikit-learn">8. <strong>Scikit-Learn</strong></h3> <ul> <li><strong>Overview</strong>: Scikit-learn is a widely-used machine learning library in Python, focused on traditional ML algorithms.</li> <li><strong>Why it‚Äôs great</strong>: It‚Äôs a great starting point to contribute to core ML algorithms. Many companies use scikit-learn in production, so contributing here could make you highly visible.</li> <li><strong>Skills you‚Äôll gain</strong>: Classical ML algorithms, Python, data science workflows.</li> <li><strong>Get started</strong>: Check out the <a href="https://scikit-learn.org/stable/developers/contributing.html">contribution guide</a> and issues labeled for new contributors.</li> </ul> <h3 id="general-tips">General Tips:</h3> <ul> <li><strong>Start Small</strong>: Begin with simple tasks like documentation updates, code review, or small bug fixes. This helps you learn the codebase and build confidence.</li> <li><strong>Join the Community</strong>: Engage with the project‚Äôs community via forums, Slack, or GitHub discussions to get help and meet other contributors.</li> <li><strong>Be Consistent</strong>: Regular contributions, no matter how small, can help you build a reputation in the community and attract collaboration opportunities.</li> </ul>]]></content><author><name></name></author><category term="open-source"/><category term="opensource"/><category term="ai"/><category term="github"/><summary type="html"><![CDATA[A quick lookup list for me to keep track of potential opensource projects]]></summary></entry><entry><title type="html">Augment E-Commerce with Deep Learning</title><link href="https://orionstar25.github.io/blog/2024/ecomm-ml/" rel="alternate" type="text/html" title="Augment E-Commerce with Deep Learning"/><published>2024-06-27T00:00:00+00:00</published><updated>2024-06-27T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/ecomm-ml</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/ecomm-ml/"><![CDATA[<p>Usually, ML interviews consist of case studies that require you to think about what features to use given a use-case. However, interviewers ocassionally also throw open-ended questions like - ‚ÄúHow will you increase the functionality of our current product using AI/ML?‚Äù. Thus, I had recently given a series of ML interviews for multiple e-commerce platforms and I did some pre-thinking before these interviews to prepare better. Below are a few ways I feel AI can be leveraged to improve the quality of e-commerce apps.</p> <h1 id="already-happening-">Already happening ‚úÖ</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Targetted ads
- Personalized offers
- Dynamic pricing
- Recommendations for increased visibility
- increased engagement
</code></pre></div></div> <h1 id="enhancements-">Enhancements üìà</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Rebalance driver fleets according to demand
	- followed by driver allocation
	- incentivize
	- lesser waiting times
	- greater coverage

- augment customer service
	- automate flow with bots
	- provide solutions based on existing knowledge base (customer profiles, previous interactions, existing solutions)
	- greater autonomy to solve problems at care-tech level than engineer level
		- analysis entire order life-cycle to pin-point issue (stuck state, driver reblast)
	- use feedbacks to filter out complaints, suggestions and direct to relevant team
		- summarize pain points
		- provide initial problem suggestions

- augment the app to a more human-like experience
	- "snacks for rainy day"
		- recommend food
			- individual items or set packages
		- provide customized offers (discount on existing cart, discount to add more items)
		- automatically add to cart, review, pay
	- "need ingredients to make biryani"
		- list products, or complete packages
	- promote (users+restos) based on festivals
	- create a mini-google within the app
		- "how much time form X to Y and whats the price"
		- review, book ride
	- spotify-like recommendations
		- "people also like"
		- "top 10 foods in your area"
		- "new releases"

- have a unified cart with multiple restos (like amazon)
	- order pizza from dominos
	- chicken from kfc
	- pay all at once
	- internally calculate respective breakup and pay merchant

- use AI for edge devices
	- "knowledge distillation," or ‚ÄúDomain reduction‚Äù, which involves training a smaller model to mimic the behavior of a larger model on a smaller data set
	- quantization to reduce memory footprint
	- hybrid solutions (cloud + edge)
		- with devices running ‚Äòlight‚Äô versions of the model for low latency while the cloud processes multiple tokens of the ‚Äòfull‚Äô model in parallel and corrects the device answers if needed.

- Robustness
</code></pre></div></div>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="e-commerce"/><category term="technology"/><summary type="html"><![CDATA[A running list of ideas of how existing e-commerce platforms can leverage AI]]></summary></entry><entry><title type="html">[WIP] The RAG Triad - Metrics to evaluate Advanced RAG</title><link href="https://orionstar25.github.io/blog/2024/evaluate-rag/" rel="alternate" type="text/html" title="[WIP] The RAG Triad - Metrics to evaluate Advanced RAG"/><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/evaluate-rag</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/evaluate-rag/"><![CDATA[<p>https://docs.google.com/presentation/d/14zE7RMad5OAp3IknnjqGdfdmKwiJObo_Wp6ikSwPxCo/edit?usp=sharing <a href="https://x.com/GDGCloudMumbai/status/1804489370801676375">Recording</a>, <a href="https://docs.google.com/presentation/d/14zE7RMad5OAp3IknnjqGdfdmKwiJObo_Wp6ikSwPxCo/edit?usp=sharing">Slides</a> https://github.com/OrionStar25/Build-and-Evaluate-RAGs</p> <p>https://huggingface.co/learn/cookbook/en/rag_evaluation https://python.langchain.com/v0.1/docs/integrations/chat/google_vertex_ai_palm/ https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks#vertex-ai-workbench</p> <p>start with:</p> <ol> <li>Steps in evaluating RAG <ul> <li>Build RAG</li> <li>Build a evaluation dataset <ul> <li>manually create dataset</li> <li>use an llm to create dataset</li> <li>use another llm to filter out relevant questions</li> </ul> </li> <li>Use another LLM as a judge to critic on evaluation set</li> </ul> </li> <li> <p>lots of things to tweak in the RAG pipeline - but there should be a proper way to evaluate its impact</p> </li> <li>dataset used is huggingface documentation <ul> <li>text + source</li> </ul> </li> <li>synthetic evaluation dataset creation <ul> <li>input: randomised context from knowledge base <ul> <li>text+metadata</li> <li>chunk documents into equal-sized chunks with overlap for continuity</li> </ul> </li> <li>output: <ul> <li>possible question from the context asked by a user <ul> <li>force llm to not mention anything about using a context to generate this question (user never had a context)</li> </ul> </li> <li>answer to the given question supported by the context.</li> </ul> </li> <li>use LLM A (Mixtral 7B-instruct) to create this set. <ul> <li>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts (8 experts).</li> <li>The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks.</li> </ul> </li> <li> <p>generate at least &gt; 200 QA pairs since half of these will get filtered out + evaluation set should be at least ~100 questions.</p> </li> <li>critique agent: <ul> <li>rate each question on several criteria</li> <li>evaluation criteria: <ul> <li>groundedness of question within context</li> <li>relevance of questions wrt to expected task</li> <li>stand-alone of questions without any context</li> </ul> </li> <li>give score 1 to 5 and remove all questions that have low score for any criteria</li> <li>use same LLM A to critique</li> </ul> </li> </ul> </li> <li>Build RAG <ul> <li>split documents using recursive split</li> <li>embed documents <ul> <li>model used: thenlper/gte-small</li> </ul> </li> <li>retrieve relevant chunks - like a search engine <ul> <li>FAISS index stores embeddings for quick retrievals of similar chunks</li> </ul> </li> <li>use retrieved contexts + query to formulate answer <ul> <li>rerank retrieved context <ul> <li>retrieve 30 docs</li> <li>rerank and output best 7</li> <li>model used: colbert-ir/colbertv2.0</li> <li>reevaluates and reorders the documents retrieved by the initial search based on their relevance to the query</li> </ul> </li> <li>answer using llm <ul> <li>LLM B (zephyr-7b-beta)</li> <li>a fine-tuned version of (mistralai/Mistral-7B-v0.1) trained to act as helpful assistants.</li> </ul> </li> </ul> </li> </ul> </li> <li>Evaluate RAG <ul> <li>use answer correctness as a metric <ul> <li>use a scoring rubric to ground the evaluation</li> </ul> </li> <li>use LLM C (chatvertexai/palm2)</li> <li>should use other metrics such as context relevance, faithfulness (groundedness) https://docs.ragas.io/en/latest/concepts/metrics/index.html</li> </ul> </li> </ol> <p>RAG Triads:</p> <ul> <li>answer relevance <ul> <li>is the answer relevant to the question</li> </ul> </li> <li>context relevance <ul> <li>is the retrieved context relevant to the question asked</li> </ul> </li> <li>groundedness <ul> <li>is the answer grounded in the context retrieved</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="rag"/><category term="evaluation"/><category term="opensource"/><category term="mistral"/><category term="generative ai"/><summary type="html"><![CDATA[Evaluate advanced RAG using meaningful metrics such as answer relevance, context relevance, and groundedness.]]></summary></entry><entry><title type="html">[WIP] Build advanced RAG architectures with Mistral-7b using LlamaIndex</title><link href="https://orionstar25.github.io/blog/2024/build-rag/" rel="alternate" type="text/html" title="[WIP] Build advanced RAG architectures with Mistral-7b using LlamaIndex"/><published>2024-06-20T00:00:00+00:00</published><updated>2024-06-20T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/build-rag</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/build-rag/"><![CDATA[<p>https://huggingface.co/learn/cookbook/en/rag_evaluation https://python.langchain.com/v0.1/docs/integrations/chat/google_vertex_ai_palm/ https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks#vertex-ai-workbench</p> <p>start with:</p> <ol> <li>what is rag? <ul> <li>how is it different than fie-tuning?</li> <li>ways of evaluating rag <ul> <li>humans</li> <li>static scores <ul> <li>precision</li> <li>recall</li> <li>bleu, rouge</li> </ul> </li> <li>use another llm <ul> <li>since broad capabilities</li> <li>varied human preferences</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>lots of things to tweak in the RAG pipeline - but there should be a proper way to evaluate its impact</p> </li> <li>dataset used is huggingface documentation <ul> <li>text + source</li> </ul> </li> <li>Build RAG <ul> <li>split documents using recursive split</li> <li>embed documents <ul> <li>model used: thenlper/gte-small</li> </ul> </li> <li>retrieve relevant chunks - like a search engine <ul> <li>FAISS index stores embeddings for quick retrievals of similar chunks</li> </ul> </li> <li>use retrieved contexts + query to formulate answer <ul> <li>rerank retrieved context <ul> <li>retrieve 30 docs</li> <li>rerank and output best 7</li> <li>model used: colbert-ir/colbertv2.0</li> <li>reevaluates and reorders the documents retrieved by the initial search based on their relevance to the query</li> </ul> </li> <li>answer using llm <ul> <li>LLM B (zephyr-7b-beta)</li> <li>a fine-tuned version of (mistralai/Mistral-7B-v0.1) trained to act as helpful assistants.</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>basic rag sentence-window rag auto retrieval rag</p>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="retrieval augmented generation"/><category term="opensource"/><category term="mistral"/><category term="llamaindex"/><category term="generative ai"/><summary type="html"><![CDATA[I explore 3 RAG architectures using opensource technologies.]]></summary></entry><entry><title type="html">Seeking mentors help navigate the Industry</title><link href="https://orionstar25.github.io/blog/2024/mentorship/" rel="alternate" type="text/html" title="Seeking mentors help navigate the Industry"/><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/mentorship</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/mentorship/"><![CDATA[<h1 id="my-past-experience-">My past experience üï•</h1> <p>I‚Äôve been a mentor myself before. The topics were mostly:</p> <ul> <li>How to be an open source contributor.</li> <li>How to utilise the existing women in STEM opportunities</li> <li>Mentoring on coding concepts.</li> </ul> <p>I learnt a lot via this process. In order to teach people something, you gotta be well-versed in it yourself. Therefore, providing mentoring to others pushed me to learn better, communicate better, and be accountable.</p> <p>These resources show my past mentorship activities:</p> <ul> <li><a href="https://orionstar25.github.io/mentoring/">Events</a></li> <li><a href="https://orionstar25.github.io/presentations/">Talks</a></li> </ul> <p>However, now I needed mentorship. And it has been quite a journey to find what I was looking for.</p> <h1 id="why-do-i-seek-mentorship-">Why do I seek mentorship? üßëüèª‚Äçüè´</h1> <blockquote> <p>Why is it ALWAYS beneficial to have mentors in life? What can you expect to gain from a mentorship experience? How do you find good mentors?</p> </blockquote> <p>I recently graduated with a Masters degree in AI. I had previously worked in the industry as a Software Engineer and my experience was extremely enriching. However, I knew my true calling was to be associated with AI. So naturally, after graduation I was brimming with ideas about my career, my goals, opportunities I wanted to be a part of - and I couldn‚Äôt wait to join a company and get started!</p> <p>However, my plans came to a standstill (more like delayed) because I happened to graduate in the thick of mass tech layoffs globally and impending recession. Moreover, I was looking for highly speciliazed roles in the AI domain (such as deep learning, AI engineer roles) which had <em>just</em> started to gain popularity commercially, thereby rendering very few <em>actual</em> core AI jobs. And on top of that, even though I had applied to jobs in all major countries, I was unable to secure any interviews owing to requiring a sponsored work visa (all countries decided to restrict immigration at the same time ü´†). Finally, I turned to India for prospective jobs and I observed that majority of the companies in India either didn‚Äôt have highly specialised AI roles yet (used basic stats and ML to get by), or the offered salaries were lesser than my expectations.</p> <p>The struggle for those 4 months was real.</p> <p>The truth is:</p> <ol> <li> <p>AI is irrefutably up and coming. You can see it, I can see it, there is no denying it. We‚Äôre literally living the AI revolution right now.</p> </li> <li> <p>This automatically means that there <em>will</em> be a high demand for core AI skills in the future. I have already started seeing a slight surge in AI jobs albeit some restrictions<span style="color:blue">**</span>.</p> </li> <li> <p>However, currently the global industry is in a transitioning phase themselves. They‚Äôre riddled with recession and how to <em>use</em> AI for their usecase (or should they even?).</p> </li> <li> <p>And due to this reason, I see most companies requiring a minimum PhD for core AI entry-level jobs. They require an expert in the field to shape how AI can be used for the company ‚û°Ô∏è a completely reasonable requirement!</p> </li> <li> <p>However, it means I have the hardest time finding a good, well-suited job. The whole experience gave me a huge reality check of what it is like right now.</p> </li> </ol> <h1 id="the-process-">The Process ‚ûø</h1> <p>Don‚Äôt get me wrong. I still have goals, just not a concrete direction at the moment. Therefore, considering my constant internal strife, I decided to seek external perspectives and their advice to learn from their experiences. With this motivation, I started to actively look for mentorship opportunities.</p> <p>I started with what I knew - googling and asking friends. However, after a few discussions it was evident that its not the best resource. On one hand, my friends are all around my age and figuring out life themselves. On the other, my family is too close to the issue. They start with an objective advice but then tend to show biases ü•≤. For e.g.:</p> <ul> <li>‚Äúyou should definitely think about pursuing a PhD. It has xyz benefits and you have the callibre for it.‚Äù</li> <li>‚Äúhowever, its a major time commitment and you gotta think about <em>other</em> things too‚Äù.</li> </ul> <p>üòë Yeah. No. Won‚Äôt work. Sorry.</p> <p>Hence, it was important I looked for someone who was firm and objective, with no biases, and would tell me exactly as it was. I personally believe, it is important to have mentors you can realistically look up to. Someone with whom you interact periodically and gain guidance from. Someone who has diverse experiences to share. I‚Äôm spelling this out for myself right now because at one point I thought I don‚Äôt really <em>need</em> mentors. I could just look up to great personas such as Elon Musk and try to follow in his footsteps. However, in hindsight, that is the most ridiculous thing ever. Elon‚Äôs work is admirable. But his circumstances were extraordinary and nothing like mine. I cannot possible read a few articles on his life story and make important career choices. It just wouldn‚Äôt translate. (maybe one day it might not be as ridiculous as today haha! üòè).</p> <p>I finally found a few leads!</p> <ol> <li> <p>I had been following the work of a Senior ML-engineer at Google for a month on LinkedIn and decided to reach out to them. They were exceptionally helpful, especially since they work in the thick of AI.</p> </li> <li> <p>I signed up on a website called <a href="https://www.vlookup.ai/">vLookUp</a> which is a platform to connect rising career women with prospective mentors. Via this, I got connected to a professional who has been in the industry for decades. They understood my worries and shared great stories about their experiences - how they witnessed the big Windows OS, SQL boom and how it was changing the landscape of tech at the time.</p> </li> <li> <p>I approached friends of my friends who have been current data scientists for a while and heard their stories (a lot of which were funny üòÇ).</p> </li> </ol> <p>My interactions with these people is still ongoing. I still have unresolved worries but I‚Äôm trying to take it one day at a time. I will continue to seek their advice over the next coming months. But I‚Äôm so happy that I‚Äôm a step closer than yesterday üí™üèª.</p> <p>Some other resources to look for potential mentors:</p> <ul> <li><a href="https://topmate.io/">Topmate</a></li> <li><a href="https://mentorcruise.com/">MentorCruise</a></li> <li><a href="https://www.growthmentor.com/blog/online-mentoring-platforms-software/">Miscellaneous</a></li> </ul> <h1 id="some-self-introspection-Ô∏è">Some Self-Introspection üßòüèª‚Äç‚ôÄÔ∏è</h1> <p>Earlier at the start of this post, I said:</p> <blockquote> <p>And it has been quite a journey to find what I was looking for.</p> </blockquote> <p>Turns out, the journey was 95% about figuring out what <em>I wanted from life</em>, and only 5% finding the right people to guide me through it.</p> <p>I had to think long and hard about it - and somehow, it isn‚Äôt as easy as I had thought. For the longest time, I couldn‚Äôt answer the simple question: <em>‚ÄúWhat do you want to do in life?‚Äù</em>. I suspect it was a series of things that made me this way:</p> <ul> <li> <p>I‚Äôve been in a school setting for as long as I can remember. Everything is planned out, everything around you runs on a schedule. And you are expected to blindly follow it and excel in it. <strong>25 years of schooling.</strong> It obviously made me feel disoriented when this structured path was taken away from me. Now I was expected to create my own habits, my own routine, my own path - HOW FREAKIN SCARY! But also, in hindsight, how freaking nice. I get to choose.</p> </li> <li> <p>The world of tech I was familiar with had 2 specific roles: SWE and Data Scientist. I was the former and desperately wanted to be the latter. And just like that, this world transformed in a day rendering 10 different AI specific job titles with overlapping requirements, and somehow I was qualified to apply to all of them but still not enough to be considered for any. And the funniest thing is I am a Master‚Äôs graduate in AI. This <em>really</em> made me take a step back and re-evaluate life.</p> </li> </ul> <p>Every single day I learn more - about myself, about AI, about how to navigate tech. The key is to embrace and adapt, be consistent, and be present. If you don‚Äôt fit in any of the roles now, you don‚Äôt have to. I have finally realised that given the current circumstances, tech hiring is little based on skills and a lot on luck (+ strong referrals üòõ).</p> <p>As for now, I‚Äôm forging my own path. I learn and re-learn and try to keep up. I‚Äôm just trying to be consistent haha.</p> <h2 id="next-steps">Next Steps</h2> <p>This is an initial list of things I hope to achieve in the coming future:</p> <ol> <li> <p>Be more aware of the current trends in the industry. Specifically, precise trends, direction, and future prospects in the tech industry.</p> </li> <li> <p>Build connections for prospective career growth.</p> </li> <li> <p>Actively seek publick speaking opportunities. It makes me accountable and pushes my learning capabilities.</p> </li> <li> <p>Be so much better at leetcode (lol I‚Äôm super average right now ü§™).</p> </li> <li> <p>Seek external collaborations with research labs. It‚Äôs exciting to stay in touch with R&amp;D, especially right now, with blazing fast AI developments happening every week.</p> </li> <li> <p>Look for roles in AI safety and build my skills accordingly.</p> </li> </ol> <hr/> <p><em>Fin.</em></p> <p><strong>P.S.:</strong> This post will be updated as I get new insights. It‚Äôs still a WIP because I‚Äôm still a WIP üôÇ.</p> <p><span style="color:blue">**This in itself presents a plethora of interesting challenges and I plan to write a blog post dedicated to just this very soon.</span></p>]]></content><author><name></name></author><category term="industry"/><category term="mentoring"/><category term="experience"/><category term="opportunities"/><category term="artificial intelligence"/><category term="career"/><summary type="html"><![CDATA[I pen down my experiences of finding mentors to help me navigate commom industry challenges]]></summary></entry><entry><title type="html">I have finally graduated with a Master‚Äôs Degree!</title><link href="https://orionstar25.github.io/blog/2024/graduation/" rel="alternate" type="text/html" title="I have finally graduated with a Master‚Äôs Degree!"/><published>2024-01-05T00:00:00+00:00</published><updated>2024-01-05T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/graduation</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/graduation/"><![CDATA[<blockquote> <p><strong>TLDR;</strong> After 18 months of hard work, I am proud to graduate with a Master‚Äôs degree in Computing with a specialization in Artificial Intelligence!</p> </blockquote> <hr/> <p>I started my journey as a graduate student pursuing Master of Computing - AI Specialization in the National University of Singapore on 08 August 2022. Since then, I have been part of 10 highly specialized courses in Artificial Intelligence. These were an intensive mix of in-person lectures, tutorials, assignments, quizzes, projects, paper presentations, and of course exams! The whole journey was a gruelling yet truly rewarding 1.5 years of hardwork.</p> <p>Hence, I am glad to share that I have graduated with an Honours (Distinction) with a 4.4/5.0 CGPA!</p> <p>Throughout the course of my study, I saw a steady increase in my academic performance (my last semester GPA was 4.7 which made my parents a tad bit happier üòù). Moreover, I was not only exposed to state-of-the-art research in multiple AI domains but also made to get my hands dirty and understand the nitty-gritties of each concept. Consequently, I now find myself better equipped to vocalize my thoughts on technical topics to a larger audience in an informed, yet easy to grasp way. A complete list of all my courses can be found at the end of the post.</p> <hr/> <p>This journey also entailed a plethora of life lessons along the way. For starters, I shifted to a whole new country after 2 years of complete lockdown - of course it meant major adjustments! I learnt greatly about:</p> <ul> <li>adulting, making important decisions, how to manage friendships with a highly demanding academic load,</li> <li>living for yourself, importance of being kind to others,</li> <li>embracing the new culture, adjustments in international living situations,</li> <li>how not every moment of your life is going to result in peak performance,</li> <li>forgiving, forgetting, and eventually moving on to newer possibilities in life!</li> </ul> <p>I will admit that some times were tough and I caught myself being really hard on myself. However, through the constant support of my family and friends, we all saw it through ü•∞! My next steps in life are still uncertain but I am hopeful for the future. I find myself to be a fresh grad in AI in the midst of an AI revolution. There is no better time to start my professional journey.</p> <p>In conclusion, I‚Äôd like to quote a saying I always fall back to when the end goal seems too hazy or out of immediate reach.</p> <blockquote> <p>‚ÄúThe journey is more important that the destination.‚Äù - Ralph Waldo Emerson</p> </blockquote> <hr/> <h4 id="nus-course-list">NUS Course List</h4> <ul> <li>Semester 1 <ol> <li>CS4248: Natural Language Processing</li> <li>CS5242: Neural Networks and Deep Learning</li> <li>CS5340: Uncertainty Modelling in AI</li> </ol> </li> <li>Semester 2 <ol> <li>CS5228: Knowledge Discovery and Data Mining</li> <li>CS5425: Big Data Systems for Data Science</li> <li>CS5478: Intelligent Robots: Algorithms and Systems</li> <li>CS6284: Advanced Topics in Reinforcement Learning</li> </ol> </li> <li>Semester 3 <ol> <li>CS5446: AI Planning and Decision Making</li> <li>CS5562: Trustworthy Machine Learning</li> <li>CS5647: Sound and Music Computing</li> </ol> </li> </ul>]]></content><author><name></name></author><category term="masters"/><category term="experience"/><category term="artificial intelligence"/><summary type="html"><![CDATA[Reminiscing about my 1.5 year journey]]></summary></entry><entry><title type="html">Fedora Women‚Äôs Day 2019</title><link href="https://orionstar25.github.io/blog/2019/fwd-19/" rel="alternate" type="text/html" title="Fedora Women‚Äôs Day 2019"/><published>2019-11-08T00:00:00+00:00</published><updated>2019-11-08T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2019/fwd-19</id><content type="html" xml:base="https://orionstar25.github.io/blog/2019/fwd-19/"><![CDATA[<p>On 4th November, 2019 we celebrated <strong>Fedora Women‚Äôs Day</strong> at IIIT Allahabad. I, alongwith <a href="https://www.facebook.com/aadya.mishra.583">Aadya Mishra</a> co-organized this event for an audience mainly consisting of first and second year girls, pursuing either Information Technology or Electronics and Communication. Everyone had a basic grasp on simple coding concepts, but after interacting with them it was clear that very few knew about the various opportunities girls have in the technology field and open source. Hence, I decided to take this opportunity to explain to these budding minds how much fun and full of possibilities it is to be a <strong>#WomanInTech</strong>.</p> <p>In order to start preparations for this event, we decided to have an attractive poster. Our obvious choice for its design was another talented woman, <a href="https://www.facebook.com/sunidhi.kashyap.5283">Sunidhi Kashyap</a>.</p> <blockquote> <p><strong>Fedora Women‚Äôs Day (FWD) is a day of celebration to help raise awareness and thanks for the women contributors across the Fedora Project. Fedora Women‚Äôs Day marks the anniversary of the Fedora Women team. They are an opportunity for women to learn about free and open source software and jump-start their journey as a FOSS user and/or a contributor.</strong></p> </blockquote> <blockquote> <p><strong>We have a series of fun events lined-up for all of you including:</strong> <strong>~ Mini games</strong> <strong>~ Hands-on session on how to start contributing to Fedora</strong> <strong>~ Learn to deliver talks in conferences</strong> <strong>~ Fedora opportunities and networking</strong> <strong>~ FOOD and SWAG!</strong> <strong>~ and much, much more</strong></p> </blockquote> <p><img src="http://tiny.cc/omoyfz" alt=""/></p> <p>My entire aim was to use this session to motivate these girls enough to understand their options in the field of computer science and its associated technology. Hence I created an extensive 2-hour session revolving around such topics.</p> <p>I started by introducing the <strong>Fedora DI Team</strong> and its role in nurturing inclusive events across the Fedora community and raising awareness about diversity around the globe. I explained how one can be a part of the DI team and still be called an open source contributor. Next I explained what open source means and what all Fedora offers in the community. This was followed by an interactive game where everyone were to name 5 OSS they‚Äôve used in their lives till now.</p> <p>I then moved on to explain the various coding programmes Fedora is part of, namely, <strong>Outreachy and GSoC</strong>. I gave an overview of:</p> <ol> <li>Each programs‚Äô purpose and perks.</li> <li>How one can benefit with the constant mentorship and</li> <li>How inclusive and welcoming Fedora is to its beginner contributors.</li> </ol> <p>I shared my Outreachy and Flock experience, and they seemed absolutely thrilled to be a part of something so empowering. Since most of the girls‚Äô main worry was that whether they were good enough to contribute, I gave real coding examples from my Outreachy selection process so that they knew what kind of open-source issues to expect (and that they were absolutely easy to solve).</p> <p>After I had gotten them all bustling with enthusiasm and hope, I introduced them to the Women of Fedora by showing a short video. This was followed by addressing the elephant in the room: <strong>Imposter Syndrome</strong>. They were all made to understand that they‚Äôre all much more talented than they give themselves credit for. To implement this, we had a quick <strong>#IAmRemarkable</strong> session, inspired by Google wherein each girl spoke about 5 achievements of her. It made them realize they‚Äôre all full of potential.</p> <p>I then bombarded them with a whole lot of links about scholarships, conferences, and funded speaking opportunites so that they could discover their passion in their university life by networking and exploring various fileds. Since majority of the audience consisted of first years, I gave them tips as to how can they make productive use of their winter and summer break.</p> <p>The extensive session ended with feedback, general brainstorming, and was peppered with tasty refreshments. Every girl left with a lot to think about and the atmosphere was heavily postive and encouraging. I couldnt‚Äôt be more proud sharing my knowledge with these aspirants. After all I was once like them :D</p> <p>You can access the slides here: <a href="https://github.com/OrionStar25/Delivered-Conference-Talks">slides</a></p> <p><img src="https://pbs.twimg.com/media/EImgyVzU4AAnv6-.jpg" alt=""/></p>]]></content><author><name></name></author><category term="open-source"/><category term="fedora"/><category term="women"/><category term="college"/><category term="life"/><category term="jekyll"/><category term="career"/><category term="experience"/><category term="fwd"/><category term="scholarships"/><summary type="html"><![CDATA[We celebrated FWD at IIIT-Allahabad! This is the event's report.]]></summary></entry></feed>