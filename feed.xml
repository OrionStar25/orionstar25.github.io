<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://orionstar25.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://orionstar25.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-11T05:19:00+00:00</updated><id>https://orionstar25.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Augment E-Commerce with Deep Learning</title><link href="https://orionstar25.github.io/blog/2024/ecomm-ml/" rel="alternate" type="text/html" title="Augment E-Commerce with Deep Learning"/><published>2024-06-27T00:00:00+00:00</published><updated>2024-06-27T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/ecomm-ml</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/ecomm-ml/"><![CDATA[<p>Usually, ML interviews consist of case studies that require you to think about what features to use given a use-case. However, interviewers ocassionally also throw open-ended questions like - ‚ÄúHow will you increase the functionality of our current product using AI/ML?‚Äù. Thus, I had recently given a series of ML interviews for multiple e-commerce platforms and I did some pre-thinking before these interviews to prepare better. Below are a few ways I feel AI can be leveraged to improve the quality of e-commerce apps.</p> <h1 id="already-happening-">Already happening ‚úÖ</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Targetted ads
- Personalized offers
- Dynamic pricing
- Recommendations for increased visibility
- increased engagement
</code></pre></div></div> <h1 id="enhancements-">Enhancements üìà</h1> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Rebalance driver fleets according to demand
	- followed by driver allocation
	- incentivize
	- lesser waiting times
	- greater coverage

- augment customer service
	- automate flow with bots
	- provide solutions based on existing knowledge base (customer profiles, previous interactions, existing solutions)
	- greater autonomy to solve problems at care-tech level than engineer level
		- analysis entire order life-cycle to pin-point issue (stuck state, driver reblast)
	- use feedbacks to filter out complaints, suggestions and direct to relevant team
		- summarize pain points
		- provide initial problem suggestions

- augment the app to a more human-like experience
	- "snacks for rainy day"
		- recommend food
			- individual items or set packages
		- provide customized offers (discount on existing cart, discount to add more items)
		- automatically add to cart, review, pay
	- "need ingredients to make biryani"
		- list products, or complete packages
	- promote (users+restos) based on festivals
	- create a mini-google within the app
		- "how much time form X to Y and whats the price"
		- review, book ride
	- spotify-like recommendations
		- "people also like"
		- "top 10 foods in your area"
		- "new releases"

- have a unified cart with multiple restos (like amazon)
	- order pizza from dominos
	- chicken from kfc
	- pay all at once
	- internally calculate respective breakup and pay merchant

- use AI for edge devices
	- "knowledge distillation," or ‚ÄúDomain reduction‚Äù, which involves training a smaller model to mimic the behavior of a larger model on a smaller data set
	- quantization to reduce memory footprint
	- hybrid solutions (cloud + edge)
		- with devices running ‚Äòlight‚Äô versions of the model for low latency while the cloud processes multiple tokens of the ‚Äòfull‚Äô model in parallel and corrects the device answers if needed.

- Robustness
</code></pre></div></div>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="e-commerce"/><category term="technology"/><summary type="html"><![CDATA[A running list of ideas of how existing e-commerce platforms can leverage AI]]></summary></entry><entry><title type="html">[WIP] The RAG Triad - Metrics to evaluate Advanced RAG</title><link href="https://orionstar25.github.io/blog/2024/evaluate-rag/" rel="alternate" type="text/html" title="[WIP] The RAG Triad - Metrics to evaluate Advanced RAG"/><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/evaluate-rag</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/evaluate-rag/"><![CDATA[<p>https://docs.google.com/presentation/d/14zE7RMad5OAp3IknnjqGdfdmKwiJObo_Wp6ikSwPxCo/edit?usp=sharing <a href="https://x.com/GDGCloudMumbai/status/1804489370801676375">Recording</a>, <a href="https://docs.google.com/presentation/d/14zE7RMad5OAp3IknnjqGdfdmKwiJObo_Wp6ikSwPxCo/edit?usp=sharing">Slides</a> https://github.com/OrionStar25/Build-and-Evaluate-RAGs</p> <p>https://huggingface.co/learn/cookbook/en/rag_evaluation https://python.langchain.com/v0.1/docs/integrations/chat/google_vertex_ai_palm/ https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks#vertex-ai-workbench</p> <p>start with:</p> <ol> <li>what is rag? <ul> <li>how is it different than fie-tuning?</li> <li>ways of evaluating rag <ul> <li>humans</li> <li>static scores <ul> <li>precision</li> <li>recall</li> <li>bleu, rouge</li> </ul> </li> <li>use another llm <ul> <li>since broad capabilities</li> <li>varied human preferences</li> </ul> </li> </ul> </li> </ul> </li> <li>Steps in evaluating RAG <ul> <li>Build RAG</li> <li>Build a evaluation dataset <ul> <li>manually create dataset</li> <li>use an llm to create dataset</li> <li>use another llm to filter out relevant questions</li> </ul> </li> <li>Use another LLM as a judge to critic on evaluation set</li> </ul> </li> <li> <p>lots of things to tweak in the RAG pipeline - but there should be a proper way to evaluate its impact</p> </li> <li>dataset used is huggingface documentation <ul> <li>text + source</li> </ul> </li> <li>synthetic evaluation dataset creation <ul> <li>input: randomised context from knowledge base <ul> <li>text+metadata</li> <li>chunk documents into equal-sized chunks with overlap for continuity</li> </ul> </li> <li>output: <ul> <li>possible question from the context asked by a user <ul> <li>force llm to not mention anything about using a context to generate this question (user never had a context)</li> </ul> </li> <li>answer to the given question supported by the context.</li> </ul> </li> <li>use LLM A (Mixtral 7B-instruct) to create this set. <ul> <li>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts (8 experts).</li> <li>The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks.</li> </ul> </li> <li> <p>generate at least &gt; 200 QA pairs since half of these will get filtered out + evaluation set should be at least ~100 questions.</p> </li> <li>critique agent: <ul> <li>rate each question on several criteria</li> <li>evaluation criteria: <ul> <li>groundedness of question within context</li> <li>relevance of questions wrt to expected task</li> <li>stand-alone of questions without any context</li> </ul> </li> <li>give score 1 to 5 and remove all questions that have low score for any criteria</li> <li>use same LLM A to critique</li> </ul> </li> </ul> </li> <li>Build RAG <ul> <li>split documents using recursive split</li> <li>embed documents <ul> <li>model used: thenlper/gte-small</li> </ul> </li> <li>retrieve relevant chunks - like a search engine <ul> <li>FAISS index stores embeddings for quick retrievals of similar chunks</li> </ul> </li> <li>use retrieved contexts + query to formulate answer <ul> <li>rerank retrieved context <ul> <li>retrieve 30 docs</li> <li>rerank and output best 7</li> <li>model used: colbert-ir/colbertv2.0</li> <li>reevaluates and reorders the documents retrieved by the initial search based on their relevance to the query</li> </ul> </li> <li>answer using llm <ul> <li>LLM B (zephyr-7b-beta)</li> <li>a fine-tuned version of (mistralai/Mistral-7B-v0.1) trained to act as helpful assistants.</li> </ul> </li> </ul> </li> </ul> </li> <li>Evaluate RAG <ul> <li>use answer correctness as a metric <ul> <li>use a scoring rubric to ground the evaluation</li> </ul> </li> <li>use LLM C (chatvertexai/palm2)</li> <li>should use other metrics such as context relevance, faithfulness (groundedness) https://docs.ragas.io/en/latest/concepts/metrics/index.html</li> </ul> </li> </ol> <p>RAG Triads:</p> <ul> <li>answer relevance <ul> <li>is the answer relevant to the question</li> </ul> </li> <li>context relevance <ul> <li>is the retrieved context relevant to the question asked</li> </ul> </li> <li>groundedness <ul> <li>is the answer grounded in the context retrieved</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="rag"/><category term="evaluation"/><category term="opensource"/><category term="mistral"/><category term="generative ai"/><summary type="html"><![CDATA[Evaluate advanced RAG using meaningful metrics such as answer relevance, context relevance, and groundedness.]]></summary></entry><entry><title type="html">[WIP] Build advanced RAG architectures with Mistral-7b using LlamaIndex</title><link href="https://orionstar25.github.io/blog/2024/build-rag/" rel="alternate" type="text/html" title="[WIP] Build advanced RAG architectures with Mistral-7b using LlamaIndex"/><published>2024-06-20T00:00:00+00:00</published><updated>2024-06-20T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/build-rag</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/build-rag/"><![CDATA[<p>https://huggingface.co/learn/cookbook/en/rag_evaluation https://python.langchain.com/v0.1/docs/integrations/chat/google_vertex_ai_palm/ https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks#vertex-ai-workbench</p> <p>start with:</p> <ol> <li>what is rag? <ul> <li>how is it different than fie-tuning?</li> <li>ways of evaluating rag <ul> <li>humans</li> <li>static scores <ul> <li>precision</li> <li>recall</li> <li>bleu, rouge</li> </ul> </li> <li>use another llm <ul> <li>since broad capabilities</li> <li>varied human preferences</li> </ul> </li> </ul> </li> </ul> </li> <li>Steps in evaluating RAG <ul> <li>Build RAG</li> <li>Build a evaluation dataset <ul> <li>manually create dataset</li> <li>use an llm to create dataset</li> <li>use another llm to filter out relevant questions</li> </ul> </li> <li>Use another LLM as a judge to critic on evaluation set</li> </ul> </li> <li> <p>lots of things to tweak in the RAG pipeline - but there should be a proper way to evaluate its impact</p> </li> <li>dataset used is huggingface documentation <ul> <li>text + source</li> </ul> </li> <li>synthetic evaluation dataset creation <ul> <li>input: randomised context from knowledge base <ul> <li>text+metadata</li> <li>chunk documents into equal-sized chunks with overlap for continuity</li> </ul> </li> <li>output: <ul> <li>possible question from the context asked by a user <ul> <li>force llm to not mention anything about using a context to generate this question (user never had a context)</li> </ul> </li> <li>answer to the given question supported by the context.</li> </ul> </li> <li>use LLM A (Mixtral 7B-instruct) to create this set. <ul> <li>The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts (8 experts).</li> <li>The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks.</li> </ul> </li> <li> <p>generate at least &gt; 200 QA pairs since half of these will get filtered out + evaluation set should be at least ~100 questions.</p> </li> <li>critique agent: <ul> <li>rate each question on several criteria</li> <li>evaluation criteria: <ul> <li>groundedness of question within context</li> <li>relevance of questions wrt to expected task</li> <li>stand-alone of questions without any context</li> </ul> </li> <li>give score 1 to 5 and remove all questions that have low score for any criteria</li> <li>use same LLM A to critique</li> </ul> </li> </ul> </li> <li>Build RAG <ul> <li>split documents using recursive split</li> <li>embed documents <ul> <li>model used: thenlper/gte-small</li> </ul> </li> <li>retrieve relevant chunks - like a search engine <ul> <li>FAISS index stores embeddings for quick retrievals of similar chunks</li> </ul> </li> <li>use retrieved contexts + query to formulate answer <ul> <li>rerank retrieved context <ul> <li>retrieve 30 docs</li> <li>rerank and output best 7</li> <li>model used: colbert-ir/colbertv2.0</li> <li>reevaluates and reorders the documents retrieved by the initial search based on their relevance to the query</li> </ul> </li> <li>answer using llm <ul> <li>LLM B (zephyr-7b-beta)</li> <li>a fine-tuned version of (mistralai/Mistral-7B-v0.1) trained to act as helpful assistants.</li> </ul> </li> </ul> </li> </ul> </li> </ol>]]></content><author><name></name></author><category term="artificial-intelligence"/><category term="retrieval augmented generation"/><category term="opensource"/><category term="mistral"/><category term="llamaindex"/><category term="generative ai"/><summary type="html"><![CDATA[I explore 3 RAG architectures using opensource technologies.]]></summary></entry><entry><title type="html">Seeking mentors help navigate the Industry</title><link href="https://orionstar25.github.io/blog/2024/mentorship/" rel="alternate" type="text/html" title="Seeking mentors help navigate the Industry"/><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/mentorship</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/mentorship/"><![CDATA[<h1 id="my-past-experience-">My past experience üï•</h1> <p>I‚Äôve been a mentor myself before. The topics were mostly:</p> <ul> <li>How to be an open source contributor.</li> <li>How to utilise the existing women in STEM opportunities</li> <li>Mentoring on coding concepts.</li> </ul> <p>I learnt a lot via this process. In order to teach people something, you gotta be well-versed in it yourself. Therefore, providing mentoring to others pushed me to learn better, communicate better, and be accountable.</p> <p>These resources show my past mentorship activities:</p> <ul> <li><a href="https://orionstar25.github.io/mentoring/">Events</a></li> <li><a href="https://orionstar25.github.io/presentations/">Talks</a></li> </ul> <p>However, now I needed mentorship. And it has been quite a journey to find what I was looking for.</p> <h1 id="why-do-i-seek-mentorship-">Why do I seek mentorship? üßëüèª‚Äçüè´</h1> <blockquote> <p>Why is it ALWAYS beneficial to have mentors in life? What can you expect to gain from a mentorship experience? How do you find good mentors?</p> </blockquote> <p>I recently graduated with a Masters degree in AI. I had previously worked in the industry as a Software Engineer and my experience was extremely enriching. However, I knew my true calling was to be associated with AI. So naturally, after graduation I was brimming with ideas about my career, my goals, opportunities I wanted to be a part of - and I couldn‚Äôt wait to join a company and get started!</p> <p>However, my plans came to a standstill (more like delayed) because I happened to graduate in the thick of mass tech layoffs globally and impending recession. Moreover, I was looking for highly speciliazed roles in the AI domain (such as deep learning, AI engineer roles) which had <em>just</em> started to gain popularity commercially, thereby rendering very few <em>actual</em> core AI jobs. And on top of that, even though I had applied to jobs in all major countries, I was unable to secure any interviews owing to requiring a sponsored work visa (all countries decided to restrict immigration at the same time ü´†). Finally, I turned to India for prospective jobs and I observed that majority of the companies in India either didn‚Äôt have highly specialised AI roles yet (used basic stats and ML to get by), or the offered salaries were lesser than my expectations.</p> <p>The struggle for those 4 months was real.</p> <p>The truth is:</p> <ol> <li> <p>AI is irrefutably up and coming. You can see it, I can see it, there is no denying it. We‚Äôre literally living the AI revolution right now.</p> </li> <li> <p>This automatically means that there <em>will</em> be a high demand for core AI skills in the future. I have already started seeing a slight surge in AI jobs albeit some restrictions<span style="color:blue">**</span>.</p> </li> <li> <p>However, currently the global industry is in a transitioning phase themselves. They‚Äôre riddled with recession and how to <em>use</em> AI for their usecase (or should they even?).</p> </li> <li> <p>And due to this reason, I see most companies requiring a minimum PhD for core AI entry-level jobs. They require an expert in the field to shape how AI can be used for the company ‚û°Ô∏è a completely reasonable requirement!</p> </li> <li> <p>However, it means I have the hardest time finding a good, well-suited job. The whole experience gave me a huge reality check of what it is like right now.</p> </li> </ol> <h1 id="the-process-">The Process ‚ûø</h1> <p>Don‚Äôt get me wrong. I still have goals, just not a concrete direction at the moment. Therefore, considering my constant internal strife, I decided to seek external perspectives and their advice to learn from their experiences. With this motivation, I started to actively look for mentorship opportunities.</p> <p>I started with what I knew - googling and asking friends. However, after a few discussions it was evident that its not the best resource. On one hand, my friends are all around my age and figuring out life themselves. On the other, my family is too close to the issue. They start with an objective advice but then tend to show biases ü•≤. For e.g.:</p> <ul> <li>‚Äúyou should definitely think about pursuing a PhD. It has xyz benefits and you have the callibre for it.‚Äù</li> <li>‚Äúhowever, its a major time commitment and you gotta think about <em>other</em> things too‚Äù.</li> </ul> <p>üòë Yeah. No. Won‚Äôt work. Sorry.</p> <p>Hence, it was important I looked for someone who was firm and objective, with no biases, and would tell me exactly as it was. I personally believe, it is important to have mentors you can realistically look up to. Someone with whom you interact periodically and gain guidance from. Someone who has diverse experiences to share. I‚Äôm spelling this out for myself right now because at one point I thought I don‚Äôt really <em>need</em> mentors. I could just look up to great personas such as Elon Musk and try to follow in his footsteps. However, in hindsight, that is the most ridiculous thing ever. Elon‚Äôs work is admirable. But his circumstances were extraordinary and nothing like mine. I cannot possible read a few articles on his life story and make important career choices. It just wouldn‚Äôt translate. (maybe one day it might not be as ridiculous as today haha! üòè).</p> <p>I finally found a few leads!</p> <ol> <li> <p>I had been following the work of a Senior ML-engineer at Google for a month on LinkedIn and decided to reach out to them. They were exceptionally helpful, especially since they work in the thick of AI.</p> </li> <li> <p>I signed up on a website called <a href="https://www.vlookup.ai/">vLookUp</a> which is a platform to connect rising career women with prospective mentors. Via this, I got connected to a professional who has been in the industry for decades. They understood my worries and shared great stories about their experiences - how they witnessed the big Windows OS, SQL boom and how it was changing the landscape of tech at the time.</p> </li> <li> <p>I approached friends of my friends who have been current data scientists for a while and heard their stories (a lot of which were funny üòÇ).</p> </li> </ol> <p>My interactions with these people is still ongoing. I still have unresolved worries but I‚Äôm trying to take it one day at a time. I will continue to seek their advice over the next coming months. But I‚Äôm so happy that I‚Äôm a step closer than yesterday üí™üèª.</p> <p>Some other resources to look for potential mentors:</p> <ul> <li><a href="https://topmate.io/">Topmate</a></li> <li><a href="https://mentorcruise.com/">MentorCruise</a></li> <li><a href="https://www.growthmentor.com/blog/online-mentoring-platforms-software/">Miscellaneous</a></li> </ul> <h1 id="some-self-introspection-Ô∏è">Some Self-Introspection üßòüèª‚Äç‚ôÄÔ∏è</h1> <p>Earlier at the start of this post, I said:</p> <blockquote> <p>And it has been quite a journey to find what I was looking for.</p> </blockquote> <p>Turns out, the journey was 95% about figuring out what <em>I wanted from life</em>, and only 5% finding the right people to guide me through it.</p> <p>I had to think long and hard about it - and somehow, it isn‚Äôt as easy as I had thought. For the longest time, I couldn‚Äôt answer the simple question: <em>‚ÄúWhat do you want to do in life?‚Äù</em>. I suspect it was a series of things that made me this way:</p> <ul> <li> <p>I‚Äôve been in a school setting for as long as I can remember. Everything is planned out, everything around you runs on a schedule. And you are expected to blindly follow it and excel in it. <strong>25 years of schooling.</strong> It obviously made me feel disoriented when this structured path was taken away from me. Now I was expected to create my own habits, my own routine, my own path - HOW FREAKIN SCARY! But also, in hindsight, how freaking nice. I get to choose.</p> </li> <li> <p>The world of tech I was familiar with had 2 specific roles: SWE and Data Scientist. I was the former and desperately wanted to be the latter. And just like that, this world transformed in a day rendering 10 different AI specific job titles with overlapping requirements, and somehow I was qualified to apply to all of them but still not enough to be considered for any. And the funniest thing is I am a Master‚Äôs graduate in AI. This <em>really</em> made me take a step back and re-evaluate life.</p> </li> </ul> <p>Every single day I learn more - about myself, about AI, about how to navigate tech. The key is to embrace and adapt, be consistent, and be present. If you don‚Äôt fit in any of the roles now, you don‚Äôt have to. I have finally realised that given the current circumstances, tech hiring is little based on skills and a lot on luck (+ strong referrals üòõ).</p> <p>As for now, I‚Äôm forging my own path. I learn and re-learn and try to keep up. I‚Äôm just trying to be consistent haha.</p> <h2 id="next-steps">Next Steps</h2> <p>This is an initial list of things I hope to achieve in the coming future:</p> <ol> <li> <p>Be more aware of the current trends in the industry. Specifically, precise trends, direction, and future prospects in the tech industry.</p> </li> <li> <p>Build connections for prospective career growth.</p> </li> <li> <p>Actively seek publick speaking opportunities. It makes me accountable and pushes my learning capabilities.</p> </li> <li> <p>Be so much better at leetcode (lol I‚Äôm super average right now ü§™).</p> </li> <li> <p>Seek external collaborations with research labs. It‚Äôs exciting to stay in touch with R&amp;D, especially right now, with blazing fast AI developments happening every week.</p> </li> <li> <p>Look for roles in AI safety and build my skills accordingly.</p> </li> </ol> <hr/> <p><em>Fin.</em></p> <p><strong>P.S.:</strong> This post will be updated as I get new insights. It‚Äôs still a WIP because I‚Äôm still a WIP üôÇ.</p> <p><span style="color:blue">**This in itself presents a plethora of interesting challenges and I plan to write a blog post dedicated to just this very soon.</span></p>]]></content><author><name></name></author><category term="industry"/><category term="mentoring"/><category term="experience"/><category term="opportunities"/><category term="artificial intelligence"/><category term="career"/><summary type="html"><![CDATA[I pen down my experiences of finding mentors to help me navigate commom industry challenges]]></summary></entry><entry><title type="html">I have finally graduated with a Master‚Äôs Degree!</title><link href="https://orionstar25.github.io/blog/2024/graduation/" rel="alternate" type="text/html" title="I have finally graduated with a Master‚Äôs Degree!"/><published>2024-01-05T00:00:00+00:00</published><updated>2024-01-05T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2024/graduation</id><content type="html" xml:base="https://orionstar25.github.io/blog/2024/graduation/"><![CDATA[<blockquote> <p><strong>TLDR;</strong> After 18 months of hard work, I am proud to graduate with a Master‚Äôs degree in Computing with a specialization in Artificial Intelligence!</p> </blockquote> <hr/> <p>I started my journey as a graduate student pursuing Master of Computing - AI Specialization in the National University of Singapore on 08 August 2022. Since then, I have been part of 10 highly specialized courses in Artificial Intelligence. These were an intensive mix of in-person lectures, tutorials, assignments, quizzes, projects, paper presentations, and of course exams! The whole journey was a gruelling yet truly rewarding 1.5 years of hardwork.</p> <p>Hence, I am glad to share that I have graduated with an Honours (Distinction) with a 4.4/5.0 CGPA!</p> <p>Throughout the course of my study, I saw a steady increase in my academic performance (my last semester GPA was 4.7 which made my parents a tad bit happier üòù). Moreover, I was not only exposed to state-of-the-art research in multiple AI domains but also made to get my hands dirty and understand the nitty-gritties of each concept. Consequently, I now find myself better equipped to vocalize my thoughts on technical topics to a larger audience in an informed, yet easy to grasp way. A complete list of all my courses can be found at the end of the post.</p> <hr/> <p>This journey also entailed a plethora of life lessons along the way. For starters, I shifted to a whole new country after 2 years of complete lockdown - of course it meant major adjustments! I learnt greatly about:</p> <ul> <li>adulting, making important decisions, how to manage friendships with a highly demanding academic load,</li> <li>living for yourself, importance of being kind to others,</li> <li>embracing the new culture, adjustments in international living situations,</li> <li>how not every moment of your life is going to result in peak performance,</li> <li>forgiving, forgetting, and eventually moving on to newer possibilities in life!</li> </ul> <p>I will admit that some times were tough and I caught myself being really hard on myself. However, through the constant support of my family and friends, we all saw it through ü•∞! My next steps in life are still uncertain but I am hopeful for the future. I find myself to be a fresh grad in AI in the midst of an AI revolution. There is no better time to start my professional journey.</p> <p>In conclusion, I‚Äôd like to quote a saying I always fall back to when the end goal seems too hazy or out of immediate reach.</p> <blockquote> <p>‚ÄúThe journey is more important that the destination.‚Äù - Ralph Waldo Emerson</p> </blockquote> <hr/> <h4 id="nus-course-list">NUS Course List</h4> <ul> <li>Semester 1 <ol> <li>CS4248: Natural Language Processing</li> <li>CS5242: Neural Networks and Deep Learning</li> <li>CS5340: Uncertainty Modelling in AI</li> </ol> </li> <li>Semester 2 <ol> <li>CS5228: Knowledge Discovery and Data Mining</li> <li>CS5425: Big Data Systems for Data Science</li> <li>CS5478: Intelligent Robots: Algorithms and Systems</li> <li>CS6284: Advanced Topics in Reinforcement Learning</li> </ol> </li> <li>Semester 3 <ol> <li>CS5446: AI Planning and Decision Making</li> <li>CS5562: Trustworthy Machine Learning</li> <li>CS5647: Sound and Music Computing</li> </ol> </li> </ul>]]></content><author><name></name></author><category term="masters"/><category term="experience"/><category term="artificial intelligence"/><summary type="html"><![CDATA[Reminiscing about my 1.5 year journey]]></summary></entry><entry><title type="html">Fedora Women‚Äôs Day 2019</title><link href="https://orionstar25.github.io/blog/2019/fwd-19/" rel="alternate" type="text/html" title="Fedora Women‚Äôs Day 2019"/><published>2019-11-08T00:00:00+00:00</published><updated>2019-11-08T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2019/fwd-19</id><content type="html" xml:base="https://orionstar25.github.io/blog/2019/fwd-19/"><![CDATA[<p>On 4th November, 2019 we celebrated <strong>Fedora Women‚Äôs Day</strong> at IIIT Allahabad. I, alongwith <a href="https://www.facebook.com/aadya.mishra.583">Aadya Mishra</a> co-organized this event for an audience mainly consisting of first and second year girls, pursuing either Information Technology or Electronics and Communication. Everyone had a basic grasp on simple coding concepts, but after interacting with them it was clear that very few knew about the various opportunities girls have in the technology field and open source. Hence, I decided to take this opportunity to explain to these budding minds how much fun and full of possibilities it is to be a <strong>#WomanInTech</strong>.</p> <p>In order to start preparations for this event, we decided to have an attractive poster. Our obvious choice for its design was another talented woman, <a href="https://www.facebook.com/sunidhi.kashyap.5283">Sunidhi Kashyap</a>.</p> <blockquote> <p><strong>Fedora Women‚Äôs Day (FWD) is a day of celebration to help raise awareness and thanks for the women contributors across the Fedora Project. Fedora Women‚Äôs Day marks the anniversary of the Fedora Women team. They are an opportunity for women to learn about free and open source software and jump-start their journey as a FOSS user and/or a contributor.</strong></p> </blockquote> <blockquote> <p><strong>We have a series of fun events lined-up for all of you including:</strong> <strong>~ Mini games</strong> <strong>~ Hands-on session on how to start contributing to Fedora</strong> <strong>~ Learn to deliver talks in conferences</strong> <strong>~ Fedora opportunities and networking</strong> <strong>~ FOOD and SWAG!</strong> <strong>~ and much, much more</strong></p> </blockquote> <p><img src="http://tiny.cc/omoyfz" alt=""/></p> <p>My entire aim was to use this session to motivate these girls enough to understand their options in the field of computer science and its associated technology. Hence I created an extensive 2-hour session revolving around such topics.</p> <p>I started by introducing the <strong>Fedora DI Team</strong> and its role in nurturing inclusive events across the Fedora community and raising awareness about diversity around the globe. I explained how one can be a part of the DI team and still be called an open source contributor. Next I explained what open source means and what all Fedora offers in the community. This was followed by an interactive game where everyone were to name 5 OSS they‚Äôve used in their lives till now.</p> <p>I then moved on to explain the various coding programmes Fedora is part of, namely, <strong>Outreachy and GSoC</strong>. I gave an overview of:</p> <ol> <li>Each programs‚Äô purpose and perks.</li> <li>How one can benefit with the constant mentorship and</li> <li>How inclusive and welcoming Fedora is to its beginner contributors.</li> </ol> <p>I shared my Outreachy and Flock experience, and they seemed absolutely thrilled to be a part of something so empowering. Since most of the girls‚Äô main worry was that whether they were good enough to contribute, I gave real coding examples from my Outreachy selection process so that they knew what kind of open-source issues to expect (and that they were absolutely easy to solve).</p> <p>After I had gotten them all bustling with enthusiasm and hope, I introduced them to the Women of Fedora by showing a short video. This was followed by addressing the elephant in the room: <strong>Imposter Syndrome</strong>. They were all made to understand that they‚Äôre all much more talented than they give themselves credit for. To implement this, we had a quick <strong>#IAmRemarkable</strong> session, inspired by Google wherein each girl spoke about 5 achievements of her. It made them realize they‚Äôre all full of potential.</p> <p>I then bombarded them with a whole lot of links about scholarships, conferences, and funded speaking opportunites so that they could discover their passion in their university life by networking and exploring various fileds. Since majority of the audience consisted of first years, I gave them tips as to how can they make productive use of their winter and summer break.</p> <p>The extensive session ended with feedback, general brainstorming, and was peppered with tasty refreshments. Every girl left with a lot to think about and the atmosphere was heavily postive and encouraging. I couldnt‚Äôt be more proud sharing my knowledge with these aspirants. After all I was once like them :D</p> <p>You can access the slides here: <a href="https://github.com/OrionStar25/Delivered-Conference-Talks">slides</a></p> <p><img src="https://pbs.twimg.com/media/EImgyVzU4AAnv6-.jpg" alt=""/></p>]]></content><author><name></name></author><category term="open-source"/><category term="fedora"/><category term="women"/><category term="college"/><category term="life"/><category term="jekyll"/><category term="career"/><category term="experience"/><category term="fwd"/><category term="scholarships"/><summary type="html"><![CDATA[We celebrated FWD at IIIT-Allahabad! This is the event's report.]]></summary></entry><entry><title type="html">IT‚ÄôS A WRAP!</title><link href="https://orionstar25.github.io/blog/2019/outreachy-week-13/" rel="alternate" type="text/html" title="IT‚ÄôS A WRAP!"/><published>2019-08-22T00:00:00+00:00</published><updated>2019-08-22T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2019/outreachy-week-13</id><content type="html" xml:base="https://orionstar25.github.io/blog/2019/outreachy-week-13/"><![CDATA[<blockquote> <p>üé∂ <em>Country roads</em> <br/> <em>Take me home</em> <br/> <em>To the place</em> <br/> <em>I belong</em> <br/> <em>West Virginia</em> <br/> <em>Mountain Momma</em> <br/> <em>Take me home</em> <br/> <em>Country Roads</em> üé∂</p> </blockquote> <p>In this song, <code class="language-plaintext highlighter-rouge">Country roads == Open source</code>; <code class="language-plaintext highlighter-rouge">Home == Outreachy + Fedora family</code>; <code class="language-plaintext highlighter-rouge">Mountain == humongous amount of knowledge I gained</code>.</p> <p>The last 3 months of my life have not been a breeze, they‚Äôve been a whirlpool and I have been in the exact centre of it! I was finishing tasks as and when they came, was meeting new people almost everyday, all while away in a new city‚Ä¶.</p> <blockquote> <p>But the moment I used to open <a href="https://github.com/fedora-modularity/libmodulemd">libmodulemd‚Äôs</a> Github and log into IRC, I felt at home.</p> </blockquote> <p>I started as a self doubting kid and I‚Äôm going away as a confident, full-of-dreams kid. Outreachy has made me CAPABLE. I no longer use clip-on wings, because I‚Äôve grown a pair (and that too very shiny ones)! There are a lot of Open Source programs out there, but NONE of them compare to Outreachy for me. Not even GSoC. Every single thing I did in this internship was purely out of passion and the will to get better holistically. I <em>wanted</em> to write blogs, chat with mentors, learn new technology, and network with people.</p> <p>AND OUTREACHY MADE SURE I DID. They:</p> <ol> <li> <p>Sent regular bi-weekly email prompts to write blogs which also provided fantastic topics + tips to write blogs on.</p> </li> <li> <p>Had assignments for mentors too! They DEFINITELY wanted us to fully squeeze this opportunity and enable us in every way possible.</p> </li> <li> <p>Held regular Outreachy chats including all participants, mentors, and alumni on topics like: career, project progress, making connections, etc.</p> </li> <li> <p>I recently heard someone stating that GSoC is where all the ‚Äúreal talent‚Äù is, and Outreachy is mere ‚Äúspoon-feeding‚Äù. Yes, I‚Äôve had my mentor walk me through some really tough roadblocks, and yes he has reviewed my code thoroughly at all times. But THAT‚ÄôS NOT SPOON-FEEDING. That‚Äôs just good mentorship (thank you, Sir!). Outreachy projects are perfect for 3-month internships! They‚Äôre a right balance of researching and implementation. <strong>And FYI, MANY Outreachy projects are ALSO GSoC projects WITH THE SAME SET OF MENTORS!</strong></p> </li> <li> <p>Finally, provide a $500 travel stipend that you could use for conferences and tech events.</p> </li> </ol> <p>I‚Äôll admit I was quite apprehensive how I could possibly manage to dole out ‚Äúsignificant‚Äù code when I had absolutely no internship experience before. I thought I‚Äôd have to work day and night because the concept of a ‚Äúremote‚Äù internship was new to me. Thankfully, Outreachy was super flexible and not hectic at all! I used a MAMMOTH amount of concepts:</p> <ol> <li>Object C</li> <li>Unit testing in Python and Object C</li> <li>Dockers</li> <li>Still learning <a href="https://docs.python.org/3/library/unittest.mock.html">Python Mocks</a> üòú</li> <li>Git Rebase!! (This one was amazing xD)</li> <li>Python Babel</li> <li>Fedora tools like Modularity, Zanata, Koji Builds</li> <li>IRC üòÇ</li> </ol> <p>My project was divided into 3 major tasks and 1 stretch goal. And I can proudly say that <strong>I have completed all the 3 major tasks during my internship period!</strong> The library <a href="https://github.com/fedora-modularity/ModulemdTranslationHelpers">‚ÄúModulemd Translation Helpers‚Äù</a> is fully functional but just needs a little extra push to make it deployable. I:</p> <ol> <li>Converted a Modulemd Index object into Babel Catalogs.</li> <li>Converted translated Catalogs to translation documents and added them back to Index object.</li> <li>Integrated the Koji Build System for dynamic flow of metadata</li> </ol> <p>Now we‚Äôre only left to inject this new index object metadata into existing YAML by the use of an OS compose tool called Pungi. Hence then these translations will be avalaible to all Fedora users!</p> <blockquote> <p><strong>Still, the MOST AMAZING thing that has happened with me during this internship was my visit to Flock to Fedora at Budapest, Hungary and present all of this work as part of the Summer Coding Showcase.</strong></p> </blockquote> <p>I met my mentor <a href="https://twitter.com/sgallagh_redhat">Stephan Gallagher</a> and we chatted on almost everything! I met people from Panama, Czech Republic, Mexico, USA, India (obviously :P), China, Nicaragua and so many more and I discussed everything that happens there and how its different than my culture. We talked tech and how we could improve it further for people, not just pertaining to Red Hat or Fedora. I attended so many talks and workshops related to Translations, Internationalization (I20N), Modularity, and Red Hat. I also witnessed a MIND_BLOWING CANDY SWAP where people got different kinds of candies from their countries!</p> <p>Just when you thought you couldn‚Äôt possibly do EVERYTHING in 3 months, WELL I JUST DID! It‚Äôs time to move further and take these experiences with me. Its time to make those shiny wings multi-coloured üåû</p> <p><img src="https://drive.google.com/uc?export=view&amp;id=0B6ForE8OhMdTWkVkVXJVRkxFOXFhSmdURmVNX2pTTjc2cmFZ" alt="Yay"/></p>]]></content><author><name></name></author><category term="open-source"/><category term="open source"/><category term="outreachy"/><category term="technology"/><category term="gsoc"/><category term="research"/><category term="python"/><category term="jekyll"/><category term="modularity"/><category term="fedora"/><summary type="html"><![CDATA[Bye bye Outreachy, thank you for making me capable üíì]]></summary></entry><entry><title type="html">What now? #CareerGoals</title><link href="https://orionstar25.github.io/blog/2019/outreachy-week-9/" rel="alternate" type="text/html" title="What now? #CareerGoals"/><published>2019-08-21T00:00:00+00:00</published><updated>2019-08-21T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2019/outreachy-week-9</id><content type="html" xml:base="https://orionstar25.github.io/blog/2019/outreachy-week-9/"><![CDATA[<p>2019 was an absolute blast for me and it isn‚Äôt even over yet! I won a national-level hackathon, got good grades, completed TWO internships successfully, secured a job, attended my first technical conference, presented my Outreachy work in front of fellow Fedorans, gave a community talk and went to a foreign country without my parents(this one was my most commendable feat I think xD)! I have to admit I had been planning and trying out various options regarding my internships for over a year now. There were so many ‚Äúthings-should-go-this-way‚Äù timelines that I had created, and most of them were ultimately scraped off. But I can say confidently that things DID work out in the end and hardwork DID pay off and I am super grateful for that.</p> <p>So now since that huge phase of my life is over, let‚Äôs start planning the NEXT BIG phase of my life!</p> <p>I‚Äôm currently in my last year of B.Tech. According to my university rules, the last semester(Jan-May) is reserved entirely for a project of 20 credits. I could do this project:</p> <ol> <li>At a company as an industrial internship.</li> <li>Under a professor as a research internship.</li> <li>At my university as a research project.</li> </ol> <p>I am still not sure what exactly I want to do career-wise but I have some very definite ideas of how I am going to figure that out. I‚Äôve done an industrial internship at a startup. I‚Äôve done a remote open-source internship, I‚Äôve made several research projects under my university professors‚Äô guidance. So now its time for A RESEARCH INTERNSHIP! I‚Äôll keep the details very very brief(incase some potential professor sees this and wants to take me as an intern :‚Äô) )</p> <ul> <li>Are you looking for a job, internship, a grant, a volunteer position, or some type of other opportunity?</li> </ul> <blockquote> <p>Research internship abroad at a reputed university under a professor.</p> </blockquote> <ul> <li>What types of work would you like to contribute to?</li> </ul> <blockquote> <p>Research pertaining to:</p> <ol> <li>Machine learning and/or</li> <li>Deep learning and/or</li> <li>Natural Language Processing</li> </ol> </blockquote> <ul> <li>What tools or skills do you have that would help you with that work?</li> </ul> <blockquote> <p>I have 4 major projects done in that domain:</p> <ol> <li>2 in Natural Language Processing</li> <li>1 in Ensemble Learning</li> <li>1 in attention-based convolutional recurrent network <strong>all graded A+ as part of semester-long projects</strong> I am fluent with python and ML concepts, and can implement all algorithms with/without using existing frameworks. I believe in doing extensive research about various approaches befoe implementing a model.</li> </ol> </blockquote> <ul> <li>What tools or skills would you like to learn?</li> </ul> <blockquote> <p>I would like to experience exhaustive research based approaches to solutions. How to tweak models for better, faster results, and understand their nature of working.</p> </blockquote> <ul> <li>What interpersonal skills make you a collaborative team member?</li> </ul> <blockquote> <p>I listen to feedback and work accordingly. There is healthy discussion before everyone agrees on a solution. I believe in the ‚Äúgive-and-take‚Äù approach and hence shall learn and let learn when in a group.</p> </blockquote> <ul> <li>What languages do you speak, and at what school grade level?</li> </ul> <blockquote> <p>English(fluent), Hindi(fluent). 4th year of Bachelors in Information Technology.</p> </blockquote> <ul> <li>When are you available to start work?</li> </ul> <blockquote> <p>January, 2020</p> </blockquote> <ul> <li>Are you able to move, and if so, which countries/regions are you willing to move to?</li> </ul> <blockquote> <p>Anywhere which is accessible from India, will need a little funding though.</p> </blockquote> <ul> <li>Are you looking for a full-time or part-time paid position?</li> </ul> <blockquote> <p>A 6-month (semester long) research internship</p> </blockquote> <p>Here is my:</p> <ol> <li><a href="https://www.linkedin.com/in/niharikashrivastava/">LinkedIn</a></li> <li><a href="https://drive.google.com/file/d/1RdlMXxo7Vp3gNQvL0UmixyXoesODmfr4/view?usp=sharing">Resume</a></li> </ol>]]></content><author><name></name></author><category term="open source"/><category term="outreachy"/><category term="technology"/><category term="gsoc"/><category term="research"/><category term="python"/><category term="jekyll"/><category term="modularity"/><category term="fedora"/><summary type="html"><![CDATA[A piece of my current state of mind regarding my career goals]]></summary></entry><entry><title type="html">The Diary of a Gojek Summer Intern</title><link href="https://orionstar25.github.io/blog/2019/the-diary-of-a-gojek-summer-intern/" rel="alternate" type="text/html" title="The Diary of a Gojek Summer Intern"/><published>2019-08-05T03:56:51+00:00</published><updated>2019-08-05T03:56:51+00:00</updated><id>https://orionstar25.github.io/blog/2019/the-diary-of-a-gojek-summer-intern</id><content type="html" xml:base="https://orionstar25.github.io/blog/2019/the-diary-of-a-gojek-summer-intern/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Season @ Fedora Modularity</title><link href="https://orionstar25.github.io/blog/2019/outreachy-week-7/" rel="alternate" type="text/html" title="Season @ Fedora Modularity"/><published>2019-07-23T00:00:00+00:00</published><updated>2019-07-23T00:00:00+00:00</updated><id>https://orionstar25.github.io/blog/2019/outreachy-week-7</id><content type="html" xml:base="https://orionstar25.github.io/blog/2019/outreachy-week-7/"><![CDATA[<p>Hello all! It‚Äôs <strong>Open Source Summer</strong> coding season at <strong>The Fedora Project</strong>. Translations are pouring heavily at <strong>Modularity</strong>, a strong gust of <strong>Happiness Packets</strong> are incoming in from the East, <strong>Fedora pipelines</strong> are being modified to handle the frequent Fedora releases.</p> <p>I‚Äôm VERY excited to be writing this blog post. I have essentially only a month left for my Outreachy internsip with The Fedora Project to end. Surprisingly, this realization is of a bitter-sweet one. The amount of skills I‚Äôve picked up during my 2 months of Outreachy are simply priceless. I not only tried to code within deadlines, but also learnt new concepts and technology, implemeted it to reflect into my code, networked with different people as part of fedora community bonding, made MAJOR mistakes, tried to redeem myself from them (still working on that üôà), and I‚Äôm DEFINITELY not done just yet!</p> <h2 id="my-optimististic-timelines-">My Optimististic timelines üòÇ</h2> <p>Outreachy mentors and interns start the internship with a specific set of project goals. These timelines are ususally a very optimistic view of what could happen if everything goes exactly as planned. IT OFTEN DOESN‚ÄôT, but people still make optimistic plans. This concept is called as <a href="https://en.wikipedia.org/wiki/Planning_fallacy">Planning Fallacy</a>. Projects always feel easy to work on initially. But delays to projects happen. Maybe your project turned out to be more complicated than you or your mentor anticipated. Maybe you needed to learn some concepts before you could tackle project tasks. Maybe the community documention wasn‚Äôt up-to-date or was wrong.</p> <p>Keeping all this in mind, this was my initial timeline:</p> <blockquote> <p><strong>May 20, 2019 - May 25, 2019</strong> - Shall participate via community bonding. Will continuously be in touch with the mentor to break down the internship tasks into steps for the coming days. Will go through the code base thoroughly to look for vulnerabilities, potential bugs, and redundancies.</p> </blockquote> <blockquote> <p><strong>May 26, 2019 - June 10, 2019</strong> - Implement any test that exists only in Python, in C, to ensure that they are getting properly tested by the static analysis and memory-leak checkers (valgrind). This would essentially involve copying already written python tests to C.</p> </blockquote> <blockquote> <p><strong>June 11, 2019 - July 10, 2019</strong> - Write code for a set of new tests which will be provided by the mentor.</p> </blockquote> <blockquote> <p><strong>July 11, 2019 - Aug 11, 2019</strong> - Scan the library for any leftover tests, or vulnerabilities. Write exhaustive unit tests for the leftover code blocks and extend some already written tests (if necessary). Try to make the tests modular by breaking them into smaller tests for maintainability.</p> </blockquote> <blockquote> <p><strong>Aug. 12, 2019 - Aug. 20, 2019</strong> - Update documentation for all the changes made during the internship.</p> </blockquote> <p>And then my project changed completely. This time, my mentor made my new timeline:</p> <blockquote> <p><strong>Phase 1</strong>: Extract all translatable strings from the modules that have been built for each Fedora release and submit them to the translation tool, Zanata, for the translators to work on.</p> </blockquote> <blockquote> <p><strong>Phase 2</strong>: Retrieve the finished translations and use the libmodulemd API to turn them into modulemd-translations documents.</p> </blockquote> <blockquote> <p><strong>Stretch Goal</strong>: Include the code from Phase 2 into Fedora‚Äôs repo creation automation so that it gets updated automatically every day.</p> </blockquote> <p><img src="../assets/img/lifecycle_translation.png" alt="Translation Lifecycle"/></p> <p>I must say this timeline seemed very relaxed and exciting to me. This project felt like I was a consumer AND a developer for libmodulemd both at the same time.</p> <h2 id="project-flow">Project flow</h2> <p>Now that my project was broken down into tasks, I had to understand each of these problems thoroughly and break them into smaller tasks. That way, I could create a workflow and convert it into modular code. For any task I undertook, I was expected to:</p> <ol> <li>Discuss the goal of that task and do theoritical study on the required concepts.</li> <li>Write a requirements document stating: <ol> <li>All the constraints.</li> <li>Expected input/output.</li> <li>Dependencies needed(if any).</li> <li>Activity flowchart described into words</li> </ol> </li> <li>Write unit tests first for the task validating our expectation of the workflow. This is called <a href="https://www.guru99.com/test-driven-development.html">Test-Driven Development</a>.</li> <li>Finally write code and fix any intermediary bugs or add necessary features on the way.</li> <li>Ask doubts throughout, if stuck.</li> </ol> <p>Here is an extract of a Software Requirement Specification (SRS) document for my first task. You can check out the whole document <a href="https://docs.google.com/document/d/1FANPQE1qwpIjgYGIDGxNH5pOcQ4VMq9O0Xy9Ie0t_ZM/edit?ts=5ce6e6f5">here</a>.</p> <p><img src="../assets/img/SRS.png" alt="SRS Document"/></p> <h2 id="task-1">Task 1</h2> <blockquote> <p>Extract all translatable strings from the modules that have been built for each Fedora release and submit them to the translation tool, Zanata, for the translators to work on.</p> </blockquote> <p><img src="../assets/img/phase1.png" alt="Phase 1"/></p> <h3 id="extraction">Extraction</h3> <ol> <li>Fedora modular metadata is divided into modules which are stored as <code class="language-plaintext highlighter-rouge">YAML</code> files.</li> <li>Every <code class="language-plaintext highlighter-rouge">YAML</code> file is loaded into a <code class="language-plaintext highlighter-rouge">ModulemdIndex</code> object.</li> <li>Each <code class="language-plaintext highlighter-rouge">ModulemdIndex</code> object contains <code class="language-plaintext highlighter-rouge">ModulemdModule</code> objects.</li> <li>Each <code class="language-plaintext highlighter-rouge">ModulemdModule</code> object contains <code class="language-plaintext highlighter-rouge">ModulemdModuleStream</code> objects.</li> <li>Each <code class="language-plaintext highlighter-rouge">ModulemdModuleStream</code> (identified by their <a href="https://orionstar25.github.io/outreachy-week-2/">NSVCA</a>) object contains 3 translatable strings: <ol> <li>Summary</li> <li>Description</li> <li>Profile description (There may be zero or more profiles. Each profile may have a description and an arbitrary profile name.)</li> </ol> </li> </ol> <p>We stored all the strings of a <code class="language-plaintext highlighter-rouge">module:stream</code> pair that were only of the <strong>highest</strong> version. It is acceptable to assume that if there are multiple matching <code class="language-plaintext highlighter-rouge">streams</code> of the same <code class="language-plaintext highlighter-rouge">module</code>, <code class="language-plaintext highlighter-rouge">stream</code> and <code class="language-plaintext highlighter-rouge">version</code> but different <code class="language-plaintext highlighter-rouge">context</code> and/or <code class="language-plaintext highlighter-rouge">architecture</code> that you do not need to care about <code class="language-plaintext highlighter-rouge">context</code> and <code class="language-plaintext highlighter-rouge">architecture</code>. The module build system ensures that all <code class="language-plaintext highlighter-rouge">context/arches</code> share the same summary, description and profiles across the <code class="language-plaintext highlighter-rouge">version</code>.</p> <h3 id="submit-to-translation-tool">Submit to Translation tool</h3> <p><a href="https://fedora.zanata.org/?dswid=-2287">Zanata</a> is a translation tool that Fedora relies on right now for its localization needs. It accepts <a href="http://babel.pocoo.org/en/latest/api/messages/catalog.html">Babel Catalogs</a> (<code class="language-plaintext highlighter-rouge">gettext .pot</code> files) containing unique strings in one language as keys along with their locations of occurrences in the file. All these strings need to be unique in the catalog. This is important because <code class="language-plaintext highlighter-rouge">gettext .pot</code> files cannot handle having the same source string appear more than once. These catalogs are further processed into <code class="language-plaintext highlighter-rouge">.po</code> (portable object) files and forwarded for translation. Locations are of the type:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">module_name</span><span class="p">;</span><span class="n">stream_name</span><span class="p">;</span><span class="n">string_type</span>
</code></pre></div></div> <p>To put this in terms of code, there is a <code class="language-plaintext highlighter-rouge">ModulemdIndex</code> object as input, and we return a <code class="language-plaintext highlighter-rouge">Babel Catalog</code> as the output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">get_translation_catalog_from_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">project_name</span><span class="p">):</span>
	<span class="bp">...</span>
	<span class="k">return</span> <span class="n">catalog</span>
</code></pre></div></div> <p>Next, to extract the translatable strings (or just strings, for brevity):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="c1"># Get all Modulemd.Module object names
</span>    <span class="n">module_names</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">get_module_names</span><span class="p">()</span>

    <span class="c1"># Create a list containing highest version streams of a module
</span>    <span class="n">final_streams</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="n">module_names</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">get_module</span><span class="p">(</span><span class="n">module_name</span><span class="p">)</span>
        <span class="n">stream_names</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="nf">get_stream_names</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">stream_name</span> <span class="ow">in</span> <span class="n">stream_names</span><span class="p">:</span>
            <span class="c1"># The first item returned is guaranteed to be the highest version
</span>            <span class="c1"># of that stream in that module.
</span>            <span class="n">stream</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="nf">search_streams</span><span class="p">(</span><span class="n">stream_name</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">final_streams</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">stream</span><span class="p">)</span>

    <span class="c1"># A dictionary to store:
</span>    <span class="c1"># key: all translatable strings
</span>    <span class="c1"># value: their respective locations
</span>    <span class="n">translation_dict</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">stream</span> <span class="ow">in</span> <span class="n">final_streams</span><span class="p">:</span>
        <span class="c1"># Process description
</span>        <span class="n">description</span> <span class="o">=</span> <span class="n">stream</span><span class="p">.</span><span class="nf">get_description</span><span class="p">(</span><span class="sh">"</span><span class="s">C</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">location</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">{};{};description</span><span class="sh">"</span><span class="p">).</span><span class="nf">format</span><span class="p">(</span>
            <span class="n">stream</span><span class="p">.</span><span class="n">props</span><span class="p">.</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream</span><span class="p">.</span><span class="n">props</span><span class="p">.</span><span class="n">stream_name</span><span class="p">)</span>
        <span class="n">translation_dict</span><span class="p">[</span><span class="n">description</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>

        <span class="c1"># Process summary
</span>        <span class="n">summary</span> <span class="o">=</span> <span class="n">stream</span><span class="p">.</span><span class="nf">get_summary</span><span class="p">(</span><span class="sh">"</span><span class="s">C</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">location</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">{};{};summary</span><span class="sh">"</span><span class="p">).</span><span class="nf">format</span><span class="p">(</span>
            <span class="n">stream</span><span class="p">.</span><span class="n">props</span><span class="p">.</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream</span><span class="p">.</span><span class="n">props</span><span class="p">.</span><span class="n">stream_name</span><span class="p">)</span>
        <span class="n">translation_dict</span><span class="p">[</span><span class="n">summary</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>

        <span class="c1"># Process profile descriptions(sometimes NULL)
</span>        <span class="n">profile_names</span> <span class="o">=</span> <span class="n">stream</span><span class="p">.</span><span class="nf">get_profile_names</span><span class="p">()</span>
        <span class="nf">if</span><span class="p">(</span><span class="n">profile_names</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">pro_name</span> <span class="ow">in</span> <span class="n">profile_names</span><span class="p">:</span>
                <span class="n">profile</span> <span class="o">=</span> <span class="n">stream</span><span class="p">.</span><span class="nf">get_profile</span><span class="p">(</span><span class="n">pro_name</span><span class="p">)</span>

                <span class="n">profile_desc</span> <span class="o">=</span> <span class="n">profile</span><span class="p">.</span><span class="nf">get_description</span><span class="p">(</span><span class="sh">"</span><span class="s">C</span><span class="sh">"</span><span class="p">)</span>
                <span class="n">location</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">{};{};profile;{}</span><span class="sh">"</span><span class="p">).</span><span class="nf">format</span><span class="p">(</span>
                    <span class="n">stream</span><span class="p">.</span><span class="n">props</span><span class="p">.</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream</span><span class="p">.</span><span class="n">props</span><span class="p">.</span><span class="n">stream_name</span><span class="p">,</span> <span class="n">pro_name</span><span class="p">)</span>
                <span class="n">translation_dict</span><span class="p">[</span><span class="n">profile_desc</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>

</code></pre></div></div> <p>Now that we have our strings, we simply put them in our babel catalog with their location of occurrences and return it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">catalog</span> <span class="o">=</span> <span class="nc">Catalog</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">project_name</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">translatable_string</span><span class="p">,</span> <span class="n">locations</span> <span class="ow">in</span> <span class="n">translation_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">catalog</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">translatable_string</span><span class="p">,</span> <span class="n">locations</span><span class="o">=</span><span class="n">locations</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">catalog</span>
</code></pre></div></div> <h2 id="task-2">Task 2</h2> <blockquote> <p>Retrieve the finished translations and use the libmodulemd API to turn them into modulemd-translations documents.</p> </blockquote> <p><img src="../assets/img/phase2.png" alt="Phase 2"/></p> <p>After Zanata translates our strings, they provide us with a <code class="language-plaintext highlighter-rouge">.po</code> file corresponding to one language. This file is turned back into a <code class="language-plaintext highlighter-rouge">babel catalog</code> so that we can parse the translations and put them into our original <code class="language-plaintext highlighter-rouge">ModulemdIndex</code> object. This file contains headers like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Project-Id-Version: fedora-modularity-translations VERSION\n"
"POT-Creation-Date: 2018-10-16 18:39+0000\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Last-Translator: Geert Warrink &lt;geert.warrink@onsnet.nu&gt;\n"
"Language-Team: Dutch\n"
"Language: nl\n"
"X-Generator: Zanata 4.6.2\n"
</code></pre></div></div> <p>Th input is a list of such <code class="language-plaintext highlighter-rouge">catalogs</code> and our original <code class="language-plaintext highlighter-rouge">ModulemdIndex</code>. We change this <code class="language-plaintext highlighter-rouge">ModulemdIndex</code> object inplace and later this change is reflected into the corresponding <code class="language-plaintext highlighter-rouge">YAML</code> file.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_modulemd_translations_from_catalog</span><span class="p">(</span><span class="n">catalogs</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="bp">...</span>
    <span class="k">return</span> <span class="bp">None</span>
</code></pre></div></div> <p>Creating <code class="language-plaintext highlighter-rouge">ModulemdTranslation</code> objects to be later stored in our <code class="language-plaintext highlighter-rouge">index</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="c1"># Dictionary `data` contains information from catalog like:
</span>	<span class="c1"># Key: (module_name, stream_name)
</span>	<span class="c1"># Value: TranslationEntry object of a locale
</span>	<span class="n">data</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>

	<span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">catalog</span><span class="p">:</span>
		<span class="k">for</span> <span class="n">location</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">msg</span><span class="p">.</span><span class="n">locations</span><span class="p">:</span>
			<span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream_name</span><span class="p">,</span> <span class="n">string_type</span><span class="p">,</span>
				<span class="n">profile_name</span><span class="p">)</span> <span class="o">=</span> <span class="nf">split_location</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>

			<span class="k">try</span><span class="p">:</span>
				<span class="n">entry</span> <span class="o">=</span> <span class="n">data</span><span class="p">[(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream_name</span><span class="p">)]</span>
			<span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
				<span class="n">entry</span> <span class="o">=</span> <span class="n">Modulemd</span><span class="p">.</span><span class="n">TranslationEntry</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span>
					<span class="nf">str</span><span class="p">(</span><span class="n">catalog</span><span class="p">.</span><span class="n">locale</span><span class="p">))</span>

			<span class="k">if</span> <span class="n">string_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span>
				<span class="n">entry</span><span class="p">.</span><span class="nf">set_summary</span><span class="p">(</span><span class="n">msg</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>
			<span class="k">elif</span> <span class="n">string_type</span> <span class="o">==</span> <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span>
				<span class="n">entry</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="n">msg</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">entry</span><span class="p">.</span><span class="nf">set_profile_description</span><span class="p">(</span><span class="n">profile_name</span><span class="p">,</span> <span class="n">msg</span><span class="p">.</span><span class="n">string</span><span class="p">)</span>

			<span class="n">data</span><span class="p">[(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream_name</span><span class="p">)]</span> <span class="o">=</span> <span class="n">entry</span>
</code></pre></div></div> <p>And now store them in our <code class="language-plaintext highlighter-rouge">index</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">for </span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">stream_name</span><span class="p">),</span> <span class="n">mmd_translation</span> <span class="ow">in</span> <span class="n">translations</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
	<span class="k">try</span><span class="p">:</span>
		<span class="n">ret</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">add_translation</span><span class="p">(</span><span class="n">mmd_translation</span><span class="p">)</span>
	<span class="k">except</span> <span class="n">GLib</span><span class="p">.</span><span class="n">Error</span><span class="p">:</span>
		<span class="nf">print</span><span class="p">(</span>
			<span class="sh">"</span><span class="s">The translation for this %s:%s could not be added</span><span class="sh">"</span><span class="p">,</span>
			<span class="n">module_name</span><span class="p">,</span>
			<span class="n">stream_name</span><span class="p">)</span>

</code></pre></div></div> <h3 id="pr-merged-2-months-2-tasks-completed-we-were-right-on-time-till-now">PR merged! 2 months, 2 tasks completed, we were right on time till now!</h3> <h2 id="task-3">Task 3</h2> <blockquote> <p>Include the code from Phase 2 into Fedora‚Äôs repo creation automation so that it gets updated automatically every day.</p> </blockquote> <p><img src="../assets/img/phase3.png" alt="Phase 3"/></p> <p>Till now everything were just functions being called on static files. Now we would pull actual <code class="language-plaintext highlighter-rouge">YAMLs</code> from fedora‚Äôs repo creation tool, <code class="language-plaintext highlighter-rouge">Koji</code>, and then apply the process to obtain translations. I have created a CLI Tool so that individual release translations can be manually added to their respective <code class="language-plaintext highlighter-rouge">modulemd YAML</code> metadata. But to include these translations into the main release is still a work-in-progress.</p> <p>As of now, I‚Äôm a maintainer for the Modulemd Translation Helpers repository for Fedora. You can check it out <a href="https://github.com/fedora-modularity/ModulemdTranslationHelpers#modulemdtranslationhelpers">here</a>.</p> <h2 id="wrapping-up">Wrapping up</h2> <p>I still have a long way to go, but my learning curve is only getting better. I will write thorough documentation for all of my work after the completion of my project. I also presented my Outreachy project at the annual Fedora developer conference, <a href="https://flocktofedora.org/">Flock to Fedora</a> in 2019 at Budapest, Hungary. You can access the slides used for the presentation <a href="https://docs.google.com/presentation/d/1-f8p3xIJwZBc73KphAZncSC4Ykz9Zpy4eKr76_qBNts/edit?ts=5d417262#slide=id.g5ee093febd_0_32">here</a>.</p> <p>Hence proved, Outreachy is truly a rewarding experience! I hope you‚Äôd like to immerse yourself into this wonderful weather @ Fedora where the sun is about to shine soon üå¶Ô∏è.</p> <p><em>Fin.</em></p>]]></content><author><name></name></author><category term="open-source"/><category term="open source"/><category term="outreachy"/><category term="technology"/><category term="gsoc"/><category term="research"/><category term="python"/><category term="jekyll"/><category term="fedora"/><category term="modularity"/><summary type="html"><![CDATA[Progress report for my Outreachy project at Fedora Modularity]]></summary></entry></feed>